\chapter{Artifact Evaluation}\label{chap:evaluation}

This chapter presents the evaluation of the \gls{rag} pipeline developed in this work.
Sec.~\ref{sec:evaluation-metrics} describes the evaluation metrics used to assess the system's performance.
Sec.~\ref{sec:evaluation-datasets} presents the evaluation datasets created for the evaluation.
Finally, Sec.~\ref{sec:results} discusses the results of the evaluation.

\section{Evaluation Metrics}\label{sec:evaluation-metrics}
Since \gls{rag}-based systems rely on structured data retrieval, evaluating their effectiveness requires assessing both the accuracy of the retrieval and the responses generated by the \gls{llm}.
In this work, we focus on the collaborator, and consortia recommendations produced by our system using the metrics suite of the \gls{ragas} framework \cite{ragas2024}.
Optimising the performance of a \gls{rag} pipeline is particularly challenging due to its multiple components:

\begin{itemize}
    \item \textbf{Retriever component}: retrieves the relevant context from an external database to support the \gls{llm} in responding to the query.
    \item \textbf{Generator component}: produces a response based on a prompt enriched with the retrieved information.
\end{itemize}

To deeply evaluate a \gls{rag} pipeline, both components must be evaluated individually and in combination to identify areas for improvement.
\gls{ragas} is specifically designed to facilitate such evaluations, measuring the relevance of the retriever, the quality of the generator response and their overall effectiveness.
Moreover, to understand whether the performance of the \gls{rag} application is improving, it is necessary to evaluate it quantitatively.
For this, a suite of evaluation metrics and an evaluation dataset are required.
Our evaluation aims to measure the relevance and consistency of the suggested contributors, ensuring that the retrieved knowledge is in line with user demands.
The metrics used and the evaluation dataset are described below.
Finally, the results of the evaluation are discussed.

\paragraph*{Faithfulness.} It evaluates the factual consistency of the generated response with respect to the provided context.
It is derived from both the answer and the retrieved information, with scores normalized to a $(0,1)$ range, where higher values indicate better consistency.
\[
\text{Faithfulness} =
\frac{\text{Number of claims in the response supported by the retrieved context}}
{\text{Total number of claims in the response}}
\]

\paragraph*{\gls{ar}.} It measures how well the generated response aligns with the given prompt.
Lower scores indicate incomplete or redundant answers, while higher scores reflect better relevance.
It is computed as the mean cosine similarity between the original question and artificially generated questions derived from the answer.
Though typically ranging from $0$ to $1$, the score is not strictly bounded due to cosine similarity's $-1$ to $1$ range.
\[
\text{\gls{ar}} =
\frac{1}{N} \sum_{i=1}^{N} \frac{E_{g_i} \cdot E_o}{\|E_{g_i}\| \|E_o\|}
\]
where:
\begin{itemize}
    \item $E_{g_i}$ is the embedding of the generated question $i$.
    \item $E_o$ is the embedding of the original question.
    \item $N$ is the number of generated questions, which is 3 by default.
\end{itemize}

\paragraph*{\gls{cp}.} It measures how well ground-truth relevant items appear at the top ranks in the retrieved contexts. It is computed using the question, ground truth, and retrieved contexts, with values ranging from $0$ to $1$, where higher scores indicating better precision.
\[
\text{\gls{cp}@K} =
\frac{\sum_{k=1}^{K} (\text{Precision@k} \times v_k)}
{\text{Total number of relevant items in the top } K \text{ results}}
\]

\[
\text{Precision@k} = 
\frac{\text{true positives@k}}{\text{true positives@k} + \text{false positives@k}}
\]
where $K$ is the total number of retrieved chunks, and  $v_k \in \{0,1\}$  indicates relevance at rank $k$.

\paragraph*{\gls{cr}.} It evaluates how well the retrieved context aligns with the ground-truth answer.
It is computed using the question, ground truth, and retrieved context, with values ranging from $0$ to $1$, where higher scores indicating better alignment.
Each claim in the ground-truth answer is checked for attribution to the retrieved context, with ideal recall achieved when all claims are supported.
\[
\text{\gls{cr}} =
\frac{\text{Number of claims in the reference supported by the retrieved context}}
{\text{Total number of claims in the reference}}
\]

\paragraph*{\gls{cer}.} It measures how well retrieved contexts capture entities from the ground truth.
It is defined as the fraction of entities in the ground truth that are also found in the retrieved contexts.
This metric helps assess retrieval effectiveness in entity-focused tasks.
To compute it, we use two sets: $GE$ (entities in ground truths) and $CE$ (entities in retrieved contexts).
\[
\text{\gls{cer}} = \frac{|CE \cap GE|}{|GE|}
\]

\gls{cp}, \gls{cr}, and \gls{cer} are the metrics used to evaluate the retriever component.
The other ones are used to evaluate the generator component.

\paragraph*{\gls{ss}.} It measures how closely the generated answer aligns with the ground truth.
Scores range from $0$ to $1$, with higher values indicating better alignment.
This evaluation uses a cross-encoder model to compute the similarity, providing insights into response quality.

\paragraph*{\gls{ac}.} It measures how accurately the generated answer aligns with the ground truth, with scores from $0$ to $1$, where higher values indicating better correctness.
It considers both semantic and factual similarity, combined using a weighted scheme.
A threshold can be applied to convert the score to a binary value if needed.

\section{Evaluation Datasets}\label{sec:evaluation-datasets}
A \gls{rag} pipeline is evaluated based on ground truths.
\gls{ragas} is particularly distinguished by the introduction of a ``reference-free'' evaluation approach \cite{ragas2024}.
This means that, instead of relying solely on human-annotated ground truth labels, the framework uses \glspl{llm} to perform automatic evaluations, reducing the need for manual intervention.
To evaluate the performance of a \gls{rag} pipeline, \gls{ragas} requires an evaluation dataset with the following inputs:
\begin{itemize}
    \item \textbf{question}: the user query that serves as the input to the \gls{rag} pipeline.
	\item \textbf{answer}: the response generated by the \gls{rag} pipeline.
	\item \textbf{contexts}: the retrieved contextual information from an external knowledge source that supports answering the question.
	\item \textbf{ground truths}: the human-annotated reference answer to the question.
\end{itemize}

We also integrated GPT-4o within the \gls{ragas} framework to evaluate the performance and quality of our \gls{rag} pipeline.
As mentioned earlier, our evaluation focuses on the recommendation of collaborators, and the recommendation of consortia.
For this purpose, we created two evaluation datasets, one for each recommendation type, each consisting of 21 user queries paired with 21 corresponding ground truth answers.
The datasets were created by manually annotating the ground truths for each question.
Both were created based on the actual descriptions of European projects on the EU website calls.
Fig.~\ref{fig:example-collaborators-question} and Fig.~\ref{fig:example-consortium-organisations-recommendation-question} are example questions within the evaluation datasets.
Due to their length, the ground truth responses are not included in this thesis.
However, as mentioned earlier for the codebase, for a complete inspection of the evaluation datasets, including the ground truths, a detailed repository is available on GitHub at \url{https://github.com/Piermuz7/MasterThesisProject.git}.

\section{Results}\label{sec:results}

The evaluation of the \gls{rag} pipeline was performed using the evaluation datasets described in Section~\ref{sec:evaluation-datasets}.

\subsection*{Collaborators Recommendation Evaluation}\label{subsec:collaborators-recommendation-evaluation}
The results of the evaluation of the contributors' recommendations show several important aspects of the system's performance.
Table~\ref{table:collaborators-recommendation-evaluation-results} presents the evaluation results for each of the 21 queries in the collaborators recommendation evaluation dataset.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{|X|c|c|c|c|c|c|c|}
      \hline
      \textbf{Query} & \textbf{Faithfulness} & \textbf{\gls{ar}} & \textbf{\gls{cp}} & \textbf{\gls{cr}} & \textbf{\gls{cer}} & \textbf{\gls{ss}} & \textbf{\gls{ac}} \\
        \hline
        1 & NaN & 0.327 & 0.586 & NaN & 0.248 & 0.826 & NaN \\
        \hline
        2 & 0.091 & 0.388 & 0.123 & 0.325 & 0.351 & 0.836 & 0.709 \\
        \hline
        3 & NaN & 0.413 & 0.400 & 0.900 & 0.500 & 0.921 & 0.790 \\
        \hline
        4 & 0.000 & 0.242 & 0.509 & 0.500 & 0.157 & 0.865 & 0.611 \\
        \hline
        5 & 0.000 & 0.285 & 0.638 & 0.150 & 0.104 & 0.933 & 0.904 \\
        \hline
        6 & 0.200 & 0.333 & 0.149 & 0.500 & 0.239 & 0.914 & 0.823 \\
        \hline
        7 & 0.000 & 0.177 & 0.431 & 0.475 & 0.514 & 0.892 & 0.881 \\
        \hline
        8 & 0.024 & 0.302 & 0.275 & 0.500 & 0.242 & 0.857 & 0.826 \\
        \hline
        9 &  NaN & 0.274 & 0.597 & NaN & 0.259 & 0.793 & NaN \\
        \hline
        10 & 0.091 & 0.306 & 1.000 & NaN & 0.014 & 0.833 & 0.619 \\
        \hline
        11 & 0.255 & 0.232 & 0.342 & 0.500 & 0.296 & 0.750 & 0.806 \\
        \hline
        12 & NaN & 0.115 & 0.790 & NaN & 0.105 & 0.624 & NaN \\
        \hline
        13 & NaN & 0.262 & 0.947 & 0.000 & 0.374 & 0.787 & NaN \\
        \hline
        14 & NaN & 0.713 & 1.000 & 0.139 & 0.032 & 0.893 & NaN \\
        \hline
        15 & 0.000 & 0.323 & 1.000 & 0.400 & 0.250 & 0.861 & 0.610 \\
        \hline
        16 & NaN & 0.000 & 0.814 & 0.500 & 0.267 & 0.712 & 0.724 \\
        \hline
        17 & 0.750 & 0.000 & 0.987 & 1.000 & 0.321 & 0.715 & NaN \\
        \hline
        18 & NaN & 0.000 & 0.808 & 0.500 & 0.229 & 0.515 & 0.674 \\
        \hline
        19 & 0.115 & 0.000 & 0.867 & 0.500 & 0.706 & 0.652 & 0.763 \\
        \hline
        20 & 0.222 & 0.203 & 0.179 & 0.500 & 0.481 & 0.764 & 0.858 \\
        \hline
        21 & 0.068 & 0.336 & 0.778 & 0.500 & 0.588 & 0.668 & 0.703 \\
        \hline
    \end{tabularx}
    \caption{Collaborators Recommendation Evaluation Results}
    \label{table:collaborators-recommendation-evaluation-results}
\end{table}

The system demonstrates strong \gls{ss} and \gls{ac}, indicating that the recommendations generated are generally well formed and contextually relevant.
However, variability in faithfulness and \gls{cr} suggests that the system does not always retrieve or generate fully accurate responses based on available knowledge.
An interesting aspect of the results is the presence of \gls{nan} values in some evaluation metrics.
These missing values indicate cases where the system was unable to generate an evaluation, perhaps due to the absence of reference data for comparison or cases where the retrieval component did not return any relevant information.
Another reason why \gls{nan} values may appear is due to maximum outtput tokens of the model.
The faithfulness metric has several \gls{nan} entries, meaning that for some queries there was no reliable way to determine whether the generated answers were actually consistent with the retrieved knowledge.
In some cases, \gls{ar} scores are relatively high, meaning the recommendations align well with the user's query.
However, lower scores in certain queries indicate inconsistencies in retrieving the most appropriate collaborators.
\gls{cp} scores remain relatively stable, showing that when the system retrieves context, it often includes useful information.
However, the variation of \gls{cr} values suggest that while some responses leverage sufficient external knowledge, others may lack critical details, leading to incomplete recommendations.
The variation in \gls{cer} highlights that relevant entities are not always included in the retrieved context, potentially impacting the accuracy of collaborator suggestions.
In a few cases, high \gls{cer} values close to 1 indicate that the system successfully retrieved and incorporated key entities into the response.
Most of them, however, are low values, close to 0, and indicate that in some queries important entities were missing from the retrieved information, which may have affected the correctness of the recommendations.
However, low values near 0 suggest that in certain queries, important entities were missing from the retrieved information, which may have affacted the correctness of recommendations.

Overall, the evaluation suggests that, although the system is capable of generating meaningful and consistent recommendations, there is margin for improvement in ensuring factual consistency and retrieving a more complete context.
Improving retrieval strategies and refining mechanisms for ranking potential contributors could further improve the reliability and accuracy of recommendations.
Addressing the presence of missing ratings (\gls{nan} values) and ensuring that entity retrieval is more consistent will also be critical steps in improving system performance.

\subsection*{Consortium Organisations Recommendation Evaluation}\label{subsec:consortium-organisations-recommendation-evaluation}
The evaluation results for each of the 21 queries in the consortium organisations recommendation evaluation dataset are presented in Table~\ref{table:consortium-organisations-recommendation-evaluation-results}.
Even in this evaluation dataset, the system generally achieves high \gls{ac} and \gls{ss} in many cases, indicating that the generated recommendations align well with expected responses.
The fluctuation in faithfulness and \gls{cr} seems to indicate that the retrieved information does not always support the generated recommendations effectively, leading to inconsistencies in factual accuracy.
As analysed in the evaluation of collaborators recommendations, a key aspect of the results is the presence of \gls{nan} values in several evaluation metrics.
The faithfulness metric, in particular, shows these multiple values of \gls{nan} several times, suggesting that for some queries the retrieved context was not sufficient to verify the factual correctness of the generated recommendations.
The \gls{ar} scores vary significantly, with some queries achieving relatively high scores, while others show lower values.
This suggests that, while the system can generate relevant recommendations in many cases, there are instances where it struggles to retrieve the most appropriate organizations.
Similarly, \gls{cp} values fluctuate, with some queries achieving the maximum score (1.000), indicating highly precise retrieved information, whereas others show much lower values, suggesting irrelevant or less useful context.
\gls{cr} scores also vary widely, with several values near 1.000, indicating that for some queries, all relevant information was successfully retrieved.
\gls{cer} follows a similar trend, with high scores close to 1.000, meaning that key entities were retrieved effectively for some queries, while lower scores indicate missing relevant entities, which could negatively impact the accuracy of recommendations.
The \gls{ss} scores remain relatively high across most queries, showing that, despite some retrieval limitations, the generated responses maintain strong alignment with expected outputs in terms of meaning and coherence.
Additionally, \gls{ac} scores indicate that the system often produces correct recommendations, although \gls{nan} values and low scores suggest that correctness is not always guaranteed.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{|X|c|c|c|c|c|c|c|}
      \hline
      \textbf{Query} & \textbf{Faithfulness} & \textbf{\gls{ar}} & \textbf{\gls{cp}} & \textbf{\gls{cr}} & \textbf{\gls{cer}} & \textbf{\gls{ss}} & \textbf{\gls{ac}} \\
      \hline
        1 & 0.935 & 0.316 & 0.000 & 1.000 & 0.981 & 0.649 & 0.912 \\
        \hline
        2 & 0.484 & 0.214 & 0.000 & 0.364 & 0.443 & 0.811 & 0.953 \\
        \hline
        3 & 0.182 & 0.523 & 1.000 & 0.000 & 0.037 & 0.710 & 0.424 \\
        \hline
        4 & 0.529 & 0.582 & 0.677 & 1.000 & NaN & 0.774 & NaN \\
        \hline
        5 & NaN & 0.674 & 0.725 & 0.667 & 0.644 & 0.800 & 0.772 \\
        \hline
        6 & NaN & 0.659 & 0.967 & 0.667 & 0.241 & 0.817 & NaN \\
        \hline
        7 & NaN & 0.625 & 0.974 & 0.467 & 0.333 & 0.823 & NaN \\
        \hline
        8 & NaN & 0.624 & 0.874 & NaN & 0.700 & 0.858 & NaN \\
        \hline
        9 & NaN & 0.501 & 0.815 & 0.167 & 0.270 & 0.806 & NaN \\
        \hline
        10 & NaN & 0.618 & 0.180 & 1.000 & 0.455 & 0.839 & 0.625 \\
        \hline
        11 & 0.341 & 0.576 & 1.000 & 0.667 & 0.700 & 0.823 & NaN \\
        \hline
        12 & NaN & 0.633 & 0.858 & 0.067 & 0.121 & 0.802 & NaN \\
        \hline
        13 & NaN & 0.229 & 0.935 & 1.000 & 1.000 & 0.794 & 0.728 \\
        \hline
        14 & NaN & 0.546 & 0.970 & 0.133 & 0.077 & 0.786 & NaN \\
        \hline
        15 & 0.172 & 0.587 & 0.201 & 0.243 & 0.318 & 0.518 & NaN \\
        \hline
        16 & 0.478 & 0.616 & 1.000 & 1.000 & 0.405 & 0.894 & NaN \\
        \hline
        17 & 0.273 & 0.589 & 0.728 & 0.700 & 0.371 & 0.867 & 0.860 \\
        \hline
        18 & 0.538 & 0.624 & 0.719 & 0.268 & 0.200 & 0.776 & NaN \\
        \hline
        19 & 0.426 & 0.233 & 1.000 & 1.000 & 0.660 & 0.781 & NaN \\
        \hline
        20 & NaN & 0.396 & 0.798 & 1.000 & 0.450 & 0.657 & 0.644 \\
        \hline
        21 & NaN & 0.658 & 0.976 & 0.667 & 0.732 & 0.851 & 0.704 \\
        \hline
    \end{tabularx}
    \caption{Consortium Organisations Recommendation Evaluation Results}
    \label{table:consortium-organisations-recommendation-evaluation-results}
\end{table}

In conclusion, the results of the evaluation suggest that the system effectively generates consistent and relevant recommendations for consortia organisations.
There are several ways that could improve the reliability of recommendations, including improving the retrieval process, refining entity recognition and optimising the classification strategies of retrieved organisations.
As with the evaluation of contributor recommendations, resolving missing evaluations and increasing recall for weaker queries would further improve the overall performance of the system.