\chapter{Artifact Evaluation}\label{chap:evaluation}

In the evaluation phase, we addressed the last sub-research question:
\begin{center}
    \rqFour
\end{center}

\section{Evaluation Metrics}\label{sec:evaluation-metrics}
Since \gls{rag}-based systems rely on structured data retrieval, evaluating their effectiveness requires assessing both the accuracy of the retrieval and the responses generated by the \gls{llm}.
In this work, we focus on the collaborator, and consortia recommendations produced by our system using the metrics suite of the RAGAS framework \cite{ragas2024}.
Our evaluation aims to measure the relevance and consistency of the suggested contributors, ensuring that the retrieved knowledge is in line with user demands.
The metrics used and the results of our evaluation, are discussed as follows.

\paragraph*{Faithfulness.} It evaluates the factual consistency of the generated response with respect to the provided context.
It is derived from both the answer and the retrieved information, with scores normalized to a $(0,1)$ range, where higher values indicate better consistency.
\[
\text{Faithfulness} =
\frac{\text{Number of claims in the response supported by the retrieved context}}
{\text{Total number of claims in the response}}
\]

\paragraph*{\gls{ar}.} It measures how well the generated response aligns with the given prompt.
Lower scores indicate incomplete or redundant answers, while higher scores reflect better relevance.
It is computed as the mean cosine similarity between the original question and artificially generated questions derived from the answer.
Though typically ranging from $0$ to $1$, the score is not strictly bounded due to cosine similarity's $-1$ to $1$ range.
\[
\text{\gls{ar}} =
\frac{1}{N} \sum_{i=1}^{N} \frac{E_{g_i} \cdot E_o}{\|E_{g_i}\| \|E_o\|}
\]
where:
\begin{itemize}
    \item $E_{g_i}$ is the embedding of the generated question $i$.
    \item $E_o$ is the embedding of the original question.
    \item $N$ is the number of generated questions, which is 3 by default.
\end{itemize}

\paragraph*{\gls{cp}.} It measures how well ground-truth relevant items appear at the top ranks in the retrieved contexts. It is computed using the question, ground truth, and retrieved contexts, with values ranging from $0$ to $1$, where higher scores indicating better precision.
\[
\text{\gls{cp}@K} =
\frac{\sum_{k=1}^{K} (\text{Precision@k} \times v_k)}
{\text{Total number of relevant items in the top } K \text{ results}}
\]

\[
\text{Precision@k} = 
\frac{\text{true positives@k}}{\text{true positives@k} + \text{false positives@k}}
\]
where $K$ is the total number of retrieved chunks, and  $v_k \in \{0,1\}$  indicates relevance at rank $k$.

\paragraph*{\gls{cr}.} It evaluates how well the retrieved context aligns with the ground-truth answer.
It is computed using the question, ground truth, and retrieved context, with values ranging from $0$ to $1$, where higher scores indicating better alignment.
Each claim in the ground-truth answer is checked for attribution to the retrieved context, with ideal recall achieved when all claims are supported.
\[
\text{\gls{cr}} =
\frac{\text{Number of claims in the reference supported by the retrieved context}}
{\text{Total number of claims in the reference}}
\]

\paragraph*{\gls{cer}.} It measures how well retrieved contexts capture entities from the ground truth.
It is defined as the fraction of entities in the ground truth that are also found in the retrieved contexts.
This metric helps assess retrieval effectiveness in entity-focused tasks.
To compute it, we use two sets: $GE$ (entities in ground truths) and $CE$ (entities in retrieved contexts).
\[
\text{Context Entity Recall} = \frac{|CE \cap GE|}{|GE|}
\]

\paragraph*{\gls{ss}.} It measures how closely the generated answer aligns with the ground truth.
Scores range from $0$ to $1$, with higher values indicating better alignment.
This evaluation uses a cross-encoder model to compute the similarity, providing insights into response quality.

\paragraph*{\gls{ac}.} It measures how accurately the generated answer aligns with the ground truth, with scores from $0$ to $1$, where higher values indicating better correctness.
It considers both semantic and factual similarity, combined using a weighted scheme.
A threshold can be applied to convert the score to a binary value if needed.

\section{Evaluation Datasets}\label{sec:evaluation-datasets}

\section{Results}\label{sec:results}