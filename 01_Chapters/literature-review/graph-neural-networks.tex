TODO: change, reread

\acrlongpl{gnn} have become fundamental to \acrlong{dl} field, particularly for tasks involving non-Euclidean data structures. These models have had a significant impact on several domains, such as social network analysis, bioinformatics, recommender systems and \gls{nlp}. This literature review explores the development, key architectures, methodologies, applications and
methodologies, applications, challenges and future directions of \glspl{gnn}.

\subsection*{Historical Context and Evolution}

\subsubsection*{Early Work and Foundations}

Graphs have received success because the achievements of neural networks, for \gls{ml} tasks such as object detection and speech recognition.
Graphs are mathematical structures consisting of nodes connected by edges, making them ideal for modeling relationships and dependencies in complex systems.
Mathematically, a graph is defined as $G=(V, E)$, where $V$ represents the nodes and $E$ represents the edges. The edges in a graph can be either directed or undirected, depending on whether directional dependencies exist between the nodes.
Graphs are used to represent real-world datasets like protein-protein interaction networks, social networks, traffic forecasting, e-commerce, geographical maps, \glspl{kg} and so on.
\\\glspl{gnn} draw inspiration from \glspl{cnn}.
According to \cite{Khemani2024}, \glspl{cnn} and \glspl{rnn} are not well-suited for effectively handling graph-structured data.
\glspl{cnn} are designed for data with a grid structure, such as images, while \glspl{rnn} are tailored for sequences, like text.
Typically, arrays are used for storing text data, and matrices are used for image data. However, arrays and matrices are inadequate for dealing with graph data.
For graphs, we need a specialized technique called Graph Convolution. This technique allows deep neural networks to process graph-structured data directly, resulting in a \glspl{gnn}.
Nowadays, however there is a continuous increase in applications and domains that use complex structures such as graphs (\cite{Wu2021}).
The complexity of graphs implied not insignificant problems on existing \gls{ml} algorithms because a graph may have a variable number of nodes or edges; or it might have nodes that may have a different number of neighbours.
This last point is very important in some operation like convolution, that is easy for images but not for graphs.
Another motivation comes from graph representation learning, which aims to learn to represent graph nodes, edges or subgraphs by low-dimensional vectors (\cite{Zhou2020}).
Existing word embedding methods like DeepWalk (\cite{Perozzi2014}), node2vec (\cite{Grover2016}), LINE (\cite{Tang2015}) have achieved very good results but they have 2 important drawbacks. The first one is that there are no shared parameters in the encoder, which implies computationally inefficency (n. of parameters grows linearly with the n. of nodes);
The second drawback is that there's a lack of generalization because they cannot deal with dynamic graphs or generalize to new graphs.
\\Inspired by the success of \glspl{cnn} and \glspl{rnn}, \cite{Gori2005, Scarselli2009} have designed and developed the first \gls{gnn} model in order to operate on non-Euclidian space.
Variations of \glspl{gnn}, including \glspl{gcn}, \glspl{gat}, and GraphSAGE, have demonstrated groundbreaking performance on various \gls{dl} tasks in recent years.
\\To summarize, a \gls{gnn} aims to learn a state embedding \( h_v \in \mathbb{R}^s \) that encapsulates the neighborhood data of each node.
This state embedding \( h_v \), an $s$-dimensional vector for node \( v \), can be used to generate an output \( O_v \), such as the predicted distribution of the node label.
The predicted node label distribution (\( O_v \)) is derived from the state embedding \( h_v \) (\cite{Rong2019}).

\subsection*{GNN Learning Tasks and Training Settings}
According to \cite{Zhou2020,Wu2021}, \glspl{gnn} are versatile tools capable of performing a variety of node-level, edge-level, and graph-level tasks.
Each approach leverages different amounts and types of labeled data to extract meaningful patterns and representations from graph-structured data.

\textbf{Node-level} tasks focus on predicting or inferring properties of individual nodes within a graph. This includes node classification, where each node is classified into one of several predefined categories, and node regression, which predicts continuous values associated with nodes.
Node clustering groups nodes into clusters based on feature similarity and structural proximity, and node representation learning focuses on learning low-dimensional embeddings that preserve the graph's structural and feature information.

\textbf{Edge-level} tasks aim to predict or infer properties related to the edges in a graph.
Link prediction involves predicting the existence of an edge between two nodes and is crucial for applications such as recommending friends in social networks or predicting interactions in biological networks.
Edge classification assigns categories to edges based on the properties of the connected nodes and their relationships, while edge regression predicts continuous values associated with edges, such as the strength of interactions or the weight of connections.

\textbf{Graph-level} tasks involve making predictions or inferences about entire graphs.
Graph classification assigns a label to an entire graph, which is useful in applications like classifying molecules or categorizing social networks.
Graph regression predicts continuous values for entire graphs, such as the bioactivity of chemical compounds or the performance metrics of network designs.
Graph generation involves creating new graphs that resemble a given set of graphs and is important in applications like molecule generation for drug discovery or network topology design.

Fig. \ref{fig:gnn-graph-learning-tasks-summary} summarizes the key graph learning tasks that \glspl{gnn} can perform, highlighting the diversity and flexibility of these models in handling various types of graph-structured data.

\begin{figure}[htbp]
    \centering
 \includegraphics[width=.9\textwidth]{03_Figures/literature-review/gnn-graph-learning-tasks-summary.png}
     \rule{35em}{0.5pt}
    \caption{Graph learning tasks by \glspl{gnn} (\cite{Khemani2024})} 
 \label{fig:gnn-graph-learning-tasks-summary}
\end{figure}

\glspl{gnn} can be trained using a variety of learning paradigms, including supervised, semi-supervised, or unsupervised learning approaches, each of which leverages different methods and data availability to optimize the model for various analytical tasks on graph-structured data (\cite{Zhou2020,Wu2021}).

In \textbf{supervised learning}, models are trained using labeled data, where each input example is paired with the correct output.
For \glspl{gnn}, supervised tasks include node classification, link prediction, and graph classification, with a loss function defined based on the difference between predicted and actual labels.

\textbf{Semi-supervised learning} uses a small amount of labeled data along with a large amount of unlabeled data (\cite{Kipf2017}).
\glspl{gnn} are well-suited for this approach due to their ability to leverage graph structure and propagate label information through the network.
Semi-supervised tasks include semi-supervised node classification and semi-supervised link prediction, allowing models to make use of the vast amounts of unlabeled data, which is often easier and cheaper to obtain.

\textbf{Unsupervised learning} involves training a model without any labeled data, relying solely on the structure and inherent properties of the input data.
In \glspl{gnn}, unsupervised tasks include node clustering, graph embedding, and graph generation.
Techniques like autoencoders, where the model learns to reconstruct the input graph from compressed embeddings, or contrastive learning, where the model learns representations by contrasting positive and negative samples derived from the graph structure, are often used in unsupervised learning.

\subsection*{The Message-Passing Mechanism}
The message-passing mechanism in \glspl{gnn} is a sophisticated process that maintains graph symmetries through optimizable transformations on all graph properties, including nodes, edges, and the global context, preserving permutation invariances (\cite{Khemani2024}).
This ensures that the connectivity of the input graph remains unchanged, allowing the output to be characterized using the same adjacency list and feature vector count as the input.
However, the output graph has updated embeddings as the \gls{gnn} modifies each node, edge, and global-context representation.

At its core, the message-passing mechanism operates iteratively, enabling nodes to collect and aggregate information from their neighbors.
Here's a detailed breakdown of this process:
\begin{enumerate}
    \item \textbf{Node Representation Initialization}: each node in the graph is initialized with a feature vector. These initial representations can be based on node attributes or initialized randomly.

    \item \textbf{Message Passing Phase}: this phase involves multiple iterations, during which nodes exchange and aggregate information from their neighbors. The process unfolds as follows:
    \begin{itemize}
        \item[a.] \textbf{Message Computation}: each node generates a message to be sent to its neighboring nodes. This message is typically a function of the node's current feature vector and the feature vectors of its neighbors. Formally, for a node \(u\), the message \(m_{u}^{(k)}\) sent to its neighbors can be computed using a differentiable function such as a neural network:
        \[
        m_{u}^{(k)} = MessageFunction(h_u^{(k)}, \{h_v^{(k)}~|~v \in \mathcal{N}(u)\}, \{e_{uv}~|~v \in \mathcal{N}(u)\})
        \]
        where \(h_u^{(k)}\) is the feature vector of node \(u\) at iteration \(k\), \(\mathcal{N}(u)\) denotes the neighbors of node \(u\), and \(e_{uv}\) represents the edge features between nodes \(u\) and \(v\).
  
        \item[b.] \textbf{Message Aggregation}: each node aggregates the messages received from its neighbors. The aggregation function is typically permutation-invariant, ensuring that the order of the neighboring nodes does not affect the result. Common aggregation functions include summation, mean, or max. For a node \(u\), the aggregated message \(M_u^{(k)}\) is computed as:
        \[
        M_u^{(k)} = AGGREGATE^{(k)}(\{m_{v}^{(k)}~|~v \in \mathcal{N}(u)\})
        \]
  
        \item[c.] \textbf{Node Update}: each node updates its feature vector based on the aggregated message and its current feature vector. This update is performed using an update function, often a neural network. For a node \(u\), the updated feature vector \(h_u^{(k+1)}\) is computed as:
        \[
        h_u^{(k+1)} = UPDATE^{(k)}(h_u^{(k)}, M_u^{(k)})
        \]
    \end{itemize}

    \item \textbf{Iteration and Neighborhood Expansion}: the message-passing phase is repeated for multiple iterations. With each iteration, nodes accumulate information from increasingly distant parts of the graph. After the first iteration (\(k=1\)), each node's embedding includes information from its 1-hop neighborhood. After the second iteration (\(k=2\)), each node's embedding incorporates information from its 2-hop neighborhood, and so on. Generally, after \(k\) iterations, each node's embedding contains data from its \(k\)-hop neighborhood.

    \item \textbf{Structural and Feature-Based Information}: the information passed during message passing consists of two main components: structural information about the graph (such as the degree of nodes) and feature-based information (attributes of the nodes and edges). Each node's message is stored in the form of feature vectors, and during each iteration, these vectors are updated to reflect the aggregated information from neighboring nodes.

    \item \textbf{Readout Phase}: after completing the message-passing iterations, a readout function is applied to extract the final representation for nodes, edges, or the entire graph. This function aggregates the final node embeddings into a single representation. For graph-level tasks, the readout function might be a global pooling operation, such as summation, mean, or max over all node feature vectors.

    \item \textbf{Mathematical Formulation}: the process can be mathematically represented as:
   \[
   h_u^{(k+1)} = UPDATE^{(k)}\left(h_u^{(k)}, AGGREGATE^{(k)}\left(\{m_{v}^{(k)}~|~v \in \mathcal{N}(u)\}\right)\right)
   \]
   where \(h_u^{(k)}\) denotes the feature vector of node \(u\) at iteration \(k\), and \(m_{v}^{(k)}\) represents the message from node \(v\) at iteration \(k\). The AGGREGATE function combines the messages from the neighbors, and the UPDATE function combines this aggregated message with the node's current feature vector to produce the updated feature vector.


\end{enumerate}

The message-passing mechanism of \glspl{gnn} is illustrated Fig. \ref{fig:gnn-message-passing-mechanism}.
Here, an input graph with a set of node features \(X \in \mathbb{R}^{d \times |V|}\) is used to produce updated node embeddings.
The process begins with each node having an initial message (or feature vector).
Nodes then exchange messages with their neighbors. For instance, the B user node collects messages from its neighboring nodes, which are then aggregated.
This aggregated message is used to update B user's node feature vector, resulting in a new embedding for the B user node for the next iteration.
The diagram visually represents how messages are computed, aggregated, and updated across nodes in the graph, leading to refined node embeddings over iterations.

\begin{figure}[htbp]
    \centering
 \includegraphics[width=.9\textwidth]{03_Figures/literature-review/gnn-message-passing-mechanism.png}
     \rule{35em}{0.5pt}
    \caption{Message passing mechanism in \glspl{gnn} (\cite{Khemani2024})} 
 \label{fig:gnn-message-passing-mechanism}
\end{figure}

This mechanism enables \glspl{gnn} to effectively capture the dependencies and relationships within graph-structured data, making them powerful tools for tasks like node classification, link prediction, and graph classification.
By iteratively exchanging and aggregating information, \glspl{gnn} can learn rich, expressive representations that reflect both the structure and attributes of the graph.

\subsection*{\gls{gnn} Models}
Over the years, different models of \glspl{gnn} have been developed, each with its own potential and for specific tasks.
The following are the most important and best-known network architectures in the literature.

TODO: GCN, GAT, GraphSAGE

TODO: mention other models...

TODO: GNN for KGs

\subsubsection*{Spectral Approaches}

The breakthrough in spectral approaches marked a significant evolution in \glspl{gnn}. \cite{Bruna2013} introduced a method leveraging spectral graph theory to define convolution operations on graphs. This approach was refined by \cite{Defferrard2016}, leading to more efficient models. \cite{Kipf2017} simplified this concept further, making it more accessible and practical, thus popularizing the \gls{gcn}.

\subsubsection*{\acrfullpl{gcn}}

%\glspl{gcn} represent one of the most influential architectures in the \gls{gnn} landscape. They generalize the convolution operation to graph data, enabling the aggregation of feature information from a node's neighbors.
\glspl{gcn} are a fundamental variant of graph neural networks developed by \cite{Kipf2017}.
The convolution layers in \glspl{gcn} operate similarly to the convolution process in \glspl{cnn}.
In \glspl{cnn}, input neurons are multiplied by weights known as filters or kernels, which act as a sliding window across the image, allowing the network to learn from nearby cells.
Weight sharing involves using the same filter throughout the same layer of the image.
For instance, when \glspl{cnn} are used to distinguish between images of cats and non-cats, the same filter detects the cat's nose and ears across the image.
This concept applies to \glspl{gcn} as well, where similar filters are applied throughout the graph structure (\cite{Kipf2017}).

\glspl{gcn} operate by learning features through the analysis of neighboring nodes, mirroring the behavior of \glspl{cnn}.
However, the primary distinction between \glspl{cnn} and \glspl{gnn} lies in their application domains: \glspl{cnn} are designed to handle regular, Euclidean-ordered data, while \glspl{gnn} are a generalized form of CNNs suited for irregular, non-Euclidean data with varying numbers of node connections and unordered nodes.
They extend traditional \glspl{cnn}, which are typically used for grid-like data such as images, by performing convolution operations on graph data.
This enables \glspl{gcn} to capture and propagate information through the graph's nodes, considering both a node's features and those of its neighbors.
\glspl{gcn} have been successfully applied to various problems, including image classification (\cite{Monti2016}), traffic forecasting (\cite{Cui2020}), recommendation systems (\cite{Fan2019}) and scene graph generation (\cite{Yang_2018_ECCV}).

\glspl{gcn} consist of multiple layers, each performing convolution and aggregation steps to refine node representations.
By iteratively applying these layers, \glspl{gcn} can capture complex patterns and dependencies within the graph data.

A \gls{gcn} layer can be formally represented as:

\[ H^{(l+1)} = \sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right) \]

where:
\begin{itemize}
    \item \( \tilde{A} = A + I \) is the adjacency matrix with added self-loops.
    \item \( \tilde{D} \) is the degree matrix of \( \tilde{A} \).
    \item \( H^{(l)} \) and \( H^{(l+1)} \) are the input and output feature matrices for layer \( l \).
    \item \( W^{(l)} \) is the trainable weight matrix.
    \item \( \sigma \) is an activation function like ReLU.
\end{itemize}

\glspl{gcn} offer several advantages.
One key advantage is their ability to naturally handle graph-structured data, making them well-suited for tasks involving complex relational structures.
\glspl{gcn} leverage the connectivity information in graphs to capture and propagate node features, enabling more effective learning from the data's inherent structure.
This capability extends the powerful concept of convolution from Euclidean data, like images, to non-Euclidean data, such as graphs, allowing for a more generalized application of neural network models (\cite{Wu2021}).
Additionally, \glspl{gcn} are computationally efficient for large-scale graph data, as they utilize localized operations that limit the complexity of processing entire graphs at once.
This localized approach helps in scaling \glspl{gcn} to larger datasets and complex networks (\cite{Li2018}).

However, \glspl{gcn} also have limitations.
One major limitation is their reliance on the assumption of homophily, where connected nodes are assumed to have similar features.
This can restrict their effectiveness in graphs where such an assumption does not hold, such as in heterophilic graphs where connected nodes may have dissimilar features (\cite{Xu2019}).
Another limitation is the potential for oversmoothing, where node features become indistinguishable after several layers of convolution, leading to a loss of meaningful differentiation between nodes.
This can hinder the model's ability to capture fine-grained information in deeper architectures (\cite{Li2018}).
Additionally, \glspl{gcn} often struggle with scalability in extremely large graphs or dynamic graphs where the structure changes frequently, as the need to recompute convolutions can become computationally intensive.
Finally, \glspl{gcn} require careful tuning of hyperparameters and can be sensitive to the choice of architecture, which may limit their accessibility and ease of use for practitioners without extensive expertise in \glspl{gnn} (\cite{Wu2021}).

\subsubsection*{\acrfullpl{gat}}

\cite{Velickovic2018} introduced \glspl{gat}, which incorporate attention mechanisms to dynamically weigh the importance of neighboring nodes.
\gls{gat} is a novel neural network designed to work with graph-structured data.
It employs masked self-attentional layers to overcome the limitations of previous methods that relied on graph convolutions or their approximations.
By stacking these layers, \gls{gat} enables the model to implicitly assign different weights to various nodes in a neighborhood, allowing nodes to focus on the most relevant features of their neighbors.
This approach eliminates the need for expensive matrix operations, such as inversion, and does not require prior knowledge of the graph's structure.
\gls{gat} effectively addresses many significant limitations of spectral-based graph neural networks, making the model suitable for both inductive and transductive tasks.

Attention Mechanism:

\[ e_{ij} = \text{LeakyReLU}\left(a^T [W h_i \| W h_j]\right) \]

where \( e_{ij} \) is the attention score, \( a \) is the learnable attention vector, and \( \| \) denotes concatenation.

\[ \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})} \]

The node features are updated as:

\[ h_i' = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h_j\right) \]

Fig. \ref{fig:gat-attention-mechanism} shows the coefficients computed by the attention mechanism, parametrized by a weight vector $\vec{\mathbf{a}} \in \mathbb{R}^{2F}$ (where $F$ is the number of features in each node), applying a LeakyReLU activation.
\begin{figure}[htbp]
    \centering
 \includegraphics[width=.3\textwidth]{03_Figures/literature-review/gat-attention-mechanism.png}
     \rule{35em}{0.5pt}
    \caption{Attention mechanism in \glspl{gat} (\cite{Velickovic2018})} 
 \label{fig:gat-attention-mechanism}
\end{figure}


\glspl{gat} provide numerous advantages.
One of the main benefits is their capability to assign different importance weights to nodes within a neighborhood, enhancing the model's ability to concentrate on the most relevant features and relationships.
This is facilitated by the self-attention mechanism, which avoids the need for expensive matrix operations or prior knowledge of the graph's structure, thus making \glspl{gat} computationally efficient and versatile (\cite{Velickovic2018}).
Furthermore, \glspl{gat} can naturally manage graphs with varying node degrees and adapt to different scales, offering robustness and scalability.
The ability to learn dynamic attention weights for different nodes makes \glspl{gat} especially effective for tasks involving heterogeneous graphs and those with complex, irregular structures.

Nevertheless \glspl{gat} also have certain limitations.
A notable challenge is that the attention mechanism can be computationally demanding, especially for large-scale graphs with numerous nodes and edges, potentially causing scalability issues.
The computational complexity of the self-attention mechanism can grow with the number of nodes, making it less efficient for very large graphs compared to simpler convolutional methods (\cite{Thekumparampil2018}).
Additionally, although \glspl{gat} can focus on relevant nodes, they may suffer from overfitting, particularly when the attention mechanism excessively emphasizes specific nodes, leading to a reduction in generalization.
Furthermore, tuning the hyperparameters of the attention mechanism can be intricate and requires careful experimentation to achieve optimal performance (\cite{Lee2018}).

\subsubsection*{GraphSAGE}

\gls{graphsage} is an inductive learning framework for graph-structured data.
Unlike traditional methods that rely on transductive learning, \gls{graphsage} focuses on generalizing to unseen nodes, making it well-suited for dynamic graphs or scenarios where the graph structure evolves over time (\cite{Hamilton2017}).
This network consists of two main steps: Sampling and Aggregation, followed by a node representation update and normalization step.
Below, these steps are briefly described
Sampling, aggregating and predicting steps are illustrated in Fig. \ref{fig:graphsage-approach}.


\begin{enumerate}
    \item \textbf{Sampling}: in the sampling step, \gls{graphsage} samples a fixed-size set of neighbors for each node. This helps manage the complexity of the graph by limiting the number of neighbors considered during aggregation. Formally, for a node \(v\), a fixed-size set of neighbors \(\mathcal{N}(v)\) is sampled:
    \[
    \mathcal{N}(v) \subseteq \{u \in V~|~(v, u) \in E\}
    \]
 
    \item \textbf{Aggregation}: the aggregation step involves aggregating the feature vectors of the sampled neighbors to generate an updated representation for the node. Several aggregation functions can be used, including mean, LSTM-based, and pooling aggregators. The aggregation function is denoted as:
    \[
    h_{\mathcal{N}(v)}^{(k)} = \text{AGGREGATE}^{(k)}(\{h_u^{(k-1)}, \forall~u \in \mathcal{N}(v)\})
    \]
    where \(h_u^{(k-1)}\) is the representation of node \(u\) at the \((k-1)\)-th layer.
 
    \item \textbf{Update}: after aggregation, the node's representation is updated by combining its current representation with the aggregated neighborhood information. This is followed by a nonlinear transformation using a weight matrix and an activation function, such as ReLU:
    \[
    h_v^{(k)} = \sigma\left(W^{(k)} \cdot \text{CONCAT}(h_v^{(k-1)}, h_{\mathcal{N}(v)}^{(k)})\right)
    \]
    where \(W^{(k)}\) is the weight matrix for the \(k\)-th layer and \(\sigma\) is a nonlinear activation function.
 
    \item \textbf{Normalization}: finally, a normalization step is applied to ensure stability and prevent exploding or vanishing gradients. This is typically done using L2 normalization:
    \[
    h_v^{(k)} = \frac{h_v^{(k)}}{\|h_v^{(k)}\|}
    \]
\end{enumerate}

\begin{figure}[htbp]
    \centering
 \includegraphics[width=.9\textwidth]{03_Figures/literature-review/graphsage-approach.png}
     \rule{35em}{0.5pt}
    \caption{Visual working of the gls{graphsage} sampling and aggregation approach. (\cite{Hamilton2017})} 
 \label{fig:graphsage-approach}
\end{figure}

\gls{graphsage} has several advantages.
One key advantage is its ability to generalize to unseen nodes, making it suitable for inductive tasks.
This is particularly useful in dynamic graphs where new nodes and edges continuously emerge (\cite{Hamilton2017}).
Additionally, the sampling step significantly reduces computational complexity, enabling the model to scale to large graphs.
The flexibility of the aggregation function allows for customization based on specific applications, and the use of multiple layers facilitates the capture of higher-order neighborhood information.

Despite its advantages, \gls{graphsage} also has limitations.
The sampling process may lead to information loss, as only a subset of neighbors is considered.
This can affect the quality of the learned representations, especially in sparse graphs.
Additionally, the choice of the aggregation function and hyperparameters can significantly impact performance, requiring careful tuning.
Another limitation is that while \gls{graphsage} is designed to handle large-scale graphs, it may still face challenges with extremely large or highly dynamic graphs where the graph structure changes rapidly (\cite{Wu2021}).

\subsubsection*{\glspl{mpnn}}

\glspl{mpnn} (\cite{Gilmer2017}) formalized the message-passing framework for \glspl{gnn}. In each layer, nodes exchange messages with their neighbors and update their states.

General Framework:

\begin{enumerate}
    \item Message Function: \( m_{ij}^{(l)} = M(h_i^{(l)}, h_j^{(l)}, e_{ij}) \)
    \item Update Function: \( h_i^{(l+1)} = U\left(h_i^{(l)}, \sum_{j \in \mathcal{N}(i)} m_{ij}^{(l)}\right) \)
\end{enumerate}

where \( h_i \) and \( h_j \) are node features, and \( e_{ij} \) are edge features.

Advantages:
\begin{itemize}
    \item General and flexible framework.
    \item Can model complex dependencies and interactions.
\end{itemize}


Limitations:
\begin{itemize}
    \item High computational cost for dense graphs.
    \item Complexity in designing effective message and update functions.
\end{itemize}

\subsection*{Training Techniques and Challenges}

\subsubsection*{Mini-Batch Training}

To manage large graphs, mini-batch training techniques are employed. Subgraphs or neighborhoods are sampled in each training iteration to reduce memory consumption and improve efficiency.

\subsubsection*{Graph Partitioning}

Graph partitioning techniques like METIS and Louvain divide large graphs into smaller subgraphs that can be processed independently. This helps in parallelizing computations and managing memory usage.

Challenges:
\begin{itemize}
    \item Maintaining the integrity of graph structure during partitioning.
    \item Ensuring balanced computational load across partitions.
\end{itemize}

\subsubsection*{Optimization Algorithms}

Specialized optimizers, such as Adam and RMSprop, are used to stabilize training. Regularization techniques like dropout and weight decay help prevent overfitting.

\subsection*{Applications of \glspl{gnn}}

\subsubsection*{Social Network Analysis}

\glspl{gnn} are extensively used in social network analysis for tasks such as community detection, link prediction, and influence maximization. They model complex interactions and dependencies among users, providing insights into social dynamics.

Examples:
\begin{itemize}
    \item Community Detection: Using \glspl{gnn} to identify overlapping communities in social networks (\cite{Chen2017}).
    \item Link Prediction: Predicting future connections by learning node embeddings (\cite{Zeng2019}).
\end{itemize}


\subsubsection*{Biological Networks}

In bioinformatics, \glspl{gnn} are used to analyze molecular structures, predict protein functions, and understand biological processes. They model intricate interactions between biological entities, aiding in drug discovery and genomics.

Examples:
\begin{itemize}
    \item Protein-Protein Interaction: Predicting interactions between proteins by modeling their structural properties (\cite{Fout2017}).
    \item Drug Discovery: Identifying potential drug compounds by analyzing molecular graphs (\cite{Jin2018}).
\end{itemize}

\subsubsection*{Recommendation Systems}

\glspl{gnn} enhance recommendation systems by modeling user-item interactions. They improve the accuracy and relevance of recommendations by capturing complex relationships.

Examples:
\begin{itemize}
    \item Collaborative Filtering: Enhancing collaborative filtering with \glspl{gnn} to model user-item interactions (\cite{Wang2019}).
    \item Content-Based Recommendations: Using \glspl{gnn} to model item features and user preferences for personalized recommendations (\cite{Ying2018}).
\end{itemize}


\subsubsection*{\acrlong{nlp}}

\glspl{gnn} are applied in \glspl{nlp} for tasks like semantic parsing, machine translation, and text classification. By representing sentences or documents as graphs, \glspl{gnn} capture relationships between words or entities.

Examples:
\begin{itemize}
    \item Semantic Parsing: Modeling the syntactic structure of sentences for accurate semantic parsing (\cite{Zeng2019}).
    \item Relation Extraction: Extracting relationships between entities in text using \glspl{gnn} to model dependency trees (\cite{Sahu2019}).
\end{itemize}

\subsection*{Recent Advances and Future Directions}

\subsubsection*{Scalability}

Scalability remains a critical challenge for \glspl{gnn}. Recent advancements focus on models and techniques that handle large-scale graphs efficiently. Methods like GraphSAINT (\cite{Zeng2019}) and Cluster-GCN (\cite{Chiang2019}) emphasize sampling and partitioning strategies to enable scalable training.

Future Directions:
\begin{itemize}
    \item Distributed \glspl{gnn}: Developing distributed frameworks for training \glspl{gnn} on large-scale graphs.
    \item Efficient Sampling Techniques: Improving sampling methods to balance efficiency and information retention.
\end{itemize}


\subsubsection*{Expressiveness}

Enhancing the expressiveness of \glspl{gnn} involves designing architectures that capture more complex patterns and dependencies. Higher-order \glspl{gnn} and graph transformers are being explored to address this need.

Future Directions:
\begin{itemize}
    \item Higher-Order \glspl{gnn}: Extending \gls{gnn} architectures to capture higher-order interactions between nodes.
    Graph Transformers: Leveraging transformer models for graph data to enhance representational power.
\end{itemize}

\subsubsection*{Interpretability}

Understanding the decision-making process of \glspl{gnn} is crucial for their adoption in critical applications. Techniques like attention visualization, gradient-based methods, and node importance scores are being developed to interpret GNN predictions.

Future Directions:
\begin{itemize}
    \item Explainable \glspl{gnn}: Designing \gls{gnn} models with built-in interpretability features.
    \item Interpretable Training Methods: Developing training techniques that enhance model transparency and explainability.
\end{itemize}

\subsection*{Challenges and Limitations}

Despite the significant advancements and applications, \glspl{gnn} face several challenges and limitations:
\begin{itemize}
    \item Computational Complexity: \glspl{gnn} can be computationally intensive, especially for large and dense graphs, limiting their practical applicability.
    \item Scalability Issues: Handling extremely large-scale graphs remains challenging, requiring efficient algorithms and distributed computing frameworks.
    \item Over-smoothing: Deep \glspl{gnn} can suffer from over-smoothing, where node representations become indistinguishable after several layers, reducing model performance.
    \item Lack of Interpretability: The black-box nature of \glspl{gnn} poses challenges in understanding their decision-making process, which is crucial for sensitive applications.
    \item Limited Expressiveness: Some \gls{gnn} architectures struggle to capture long-range dependencies and complex interactions, necessitating further research into more expressive models.
\end{itemize}

\subsection*{Conclusion}

\acrlong{gnn} have revolutionized the field of \acrlong{dl} by providing powerful tools for modeling graph-structured data. They have demonstrated remarkable success across various domains, including social network analysis, bioinformatics, recommendation systems, and \gls{nlp}. Despite their advantages, \glspl{gnn} face challenges related to scalability, interpretability, and computational complexity. Ongoing research aims to address these challenges, exploring new architectures, efficient training techniques, and methods to enhance expressiveness and interpretability. The future of \glspl{gnn} looks promising, with potential breakthroughs that could further expand their applications and impact.
