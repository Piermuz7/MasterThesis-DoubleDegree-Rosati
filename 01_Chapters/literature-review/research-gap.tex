Despite significant advancements in \gls{dl} models for recommender systems, these models exhibit critical limitations in capturing the nuanced preferences and diverse textual side information of users.
They often struggle with generalizing to unseen recommendation scenarios and explaining the reasoning behind their predictions (\cite{Zhao2024}).
This lack of explainability and adaptability reduces their effectiveness in tasks requiring complex reasoning, such as research collaborator recommendations.

\glspl{llm} have emerged as transformative tools due to their exceptional natural language understanding and generation capabilities, enabling them to grasp complex patterns and provide human-like reasoning.
When combined with \gls{rag}, \glspl{llm} can overcome the limitations of \gls{dl} models by integrating external knowledge sources, reducing hallucinations, and providing contextually enriched, explainable, and accurate recommendations (\cite{Deldjoo2024}).

This combination represents a promising avenue for addressing the challenges of explainability and precision in recommending research collaborators, particularly by aligning recommendations with explicit user queries and leveraging diverse, up-to-date knowledge bases.
However, the integration of \glspl{llm} and \gls{rag} in this specific context remains underexplored, presenting an opportunity to bridge this gap and advance the field.