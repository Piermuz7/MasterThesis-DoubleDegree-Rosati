% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{Bizer2023}{inbook}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=c0ab52e226557a243a4a5277aec8acdc}{%
           family={Bizer},
           familyi={B\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c040f12898283360a9825ed80f52e777}{%
           family={Heath},
           familyi={H\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d469e0df304c98f67b55d087ea6187a1}{%
           family={Berners-Lee},
           familyi={B\bibinithyphendelim L\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{4171b215a738570aeb0161fcbb04938b}
      \strng{fullhash}{119c89bf63ca82bd819dbc4a7a4326c7}
      \strng{bibnamehash}{119c89bf63ca82bd819dbc4a7a4326c7}
      \strng{authorbibnamehash}{119c89bf63ca82bd819dbc4a7a4326c7}
      \strng{authornamehash}{4171b215a738570aeb0161fcbb04938b}
      \strng{authorfullhash}{119c89bf63ca82bd819dbc4a7a4326c7}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Linking the World's Information: Essays on Tim Berners-Lee's Invention of the World Wide Web}
      \field{edition}{1}
      \field{isbn}{9798400707940}
      \field{title}{Linked Data - The Story So Far}
      \field{year}{2023}
      \field{pages}{115\bibrangedash 143}
      \range{pages}{29}
      \verb{urlraw}
      \verb https://doi.org/10.1145/3591366.3591378
      \endverb
      \verb{url}
      \verb https://doi.org/10.1145/3591366.3591378
      \endverb
    \endentry
    \entry{Bonatti2017}{inbook}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=dc5b42f67dd8f52c7c01bc8cff0068b9}{%
           family={Bonatti},
           familyi={B\bibinitperiod},
           given={Piero},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5ddbf3af24e3802f8842794c8c86b60c}{%
           family={Kirrane},
           familyi={K\bibinitperiod},
           given={Sabrina},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a2be0ec850b04b1f5e9fe3aca6bf5226}{%
           family={Polleres},
           familyi={P\bibinitperiod},
           given={Axel},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fc54b330cc33b172e08abeb58605049e}{%
           family={Wenning},
           familyi={W\bibinitperiod},
           given={Rigo},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{3}{}{%
        {{hash=1035a571249f3d1e010125101ce5ac1c}{%
           family={Tonetta},
           familyi={T\bibinitperiod},
           given={Stefano},
           giveni={S\bibinitperiod}}}%
        {{hash=d7ecc4dd1a94df894c724fe3a8e5b141}{%
           family={Schoitsch},
           familyi={S\bibinitperiod},
           given={Erwin},
           giveni={E\bibinitperiod}}}%
        {{hash=53cb1ef47fe59c575e5bcddc62d58b42}{%
           family={Bitsch},
           familyi={B\bibinitperiod},
           given={Friedemann},
           giveni={F\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{85cfcf8f4c0109246b6a35d4b66df47a}
      \strng{fullhash}{cc596a29bdf5a14f77316c90cfa2dbbd}
      \strng{bibnamehash}{cc596a29bdf5a14f77316c90cfa2dbbd}
      \strng{authorbibnamehash}{cc596a29bdf5a14f77316c90cfa2dbbd}
      \strng{authornamehash}{85cfcf8f4c0109246b6a35d4b66df47a}
      \strng{authorfullhash}{cc596a29bdf5a14f77316c90cfa2dbbd}
      \strng{editorbibnamehash}{a2e2a5f30321410cd206931c51cd3883}
      \strng{editornamehash}{ccee70c746b4763cb2c135d89831838a}
      \strng{editorfullhash}{a2e2a5f30321410cd206931c51cd3883}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The European General Data Protection Regulation defines a set of obligations for personal data controllers and processors. Primary obligations include: obtaining explicit consent from the data subject for the processing of personal data, providing full transparency with respect to the processing, and enabling data rectification and erasure (albeit only in certain circumstances). At the core of any transparency architecture is the logging of events in relation to the processing and sharing of personal data. The logs should enable verification that data processors abide by the access and usage control policies that have been associated with the data based on the data subject's consent and the applicable regulations. In this position paper, we: (i) identify the requirements that need to be satisfied by such a transparency architecture, (ii) examine the suitability of existing logging mechanisms in light of said requirements, and (iii) present a number of open challenges and opportunities.}
      \field{booktitle}{Computer Safety, Reliability, and Security}
      \field{isbn}{978-3-319-66284-8}
      \field{title}{Transparent Personal Data Processing: The Road Ahead}
      \field{year}{2017}
      \field{pages}{337\bibrangedash 349}
      \range{pages}{13}
    \endentry
    \entry{Bordes2013}{misc}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=f1733e10bf044cbd7be63e10e5689d78}{%
           family={Bordes},
           familyi={B\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=26e026be6d5366fce878100ba03d68c1}{%
           family={Usunier},
           familyi={U\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2adf15bc75b1f81cef80be72a41d9d6b}{%
           family={Garcia-Dur√°n},
           familyi={G\bibinithyphendelim D\bibinitperiod},
           given={Alberto},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=417a25f3511d7c21e76d4a15e67dd679}{%
           family={Weston},
           familyi={W\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8bfc27968b4f5e8322fb9415a66f1aae}{%
           family={Yakhnenko},
           familyi={Y\bibinitperiod},
           given={Oksana},
           giveni={O\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{339de127d2ac97fb73a98efe7d8dc83f}
      \strng{fullhash}{b52eb6677718b34c9ca83324bd43a4f8}
      \strng{bibnamehash}{b52eb6677718b34c9ca83324bd43a4f8}
      \strng{authorbibnamehash}{b52eb6677718b34c9ca83324bd43a4f8}
      \strng{authornamehash}{339de127d2ac97fb73a98efe7d8dc83f}
      \strng{authorfullhash}{b52eb6677718b34c9ca83324bd43a4f8}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.}
      \field{title}{Translating Embeddings for Modeling Multi-relational Data}
      \field{year}{2013}
    \endentry
    \entry{Bruna2013}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=c83b564e32475f10dcc91be7e66d3e81}{%
           family={Bruna},
           familyi={B\bibinitperiod},
           given={Joan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e9fec85bbce1b087a6ebefe26e73f7bf}{%
           family={Zaremba},
           familyi={Z\bibinitperiod},
           given={Wojciech},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b31b3f03b1e1ee0eec6ff58f9a9df960}{%
           family={Szlam},
           familyi={S\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e943572d925293c5f4fab4dfa7adc78e}
      \strng{fullhash}{d02494bc1b8f17d758dbf6ebc90838de}
      \strng{bibnamehash}{d02494bc1b8f17d758dbf6ebc90838de}
      \strng{authorbibnamehash}{d02494bc1b8f17d758dbf6ebc90838de}
      \strng{authornamehash}{e943572d925293c5f4fab4dfa7adc78e}
      \strng{authorfullhash}{d02494bc1b8f17d758dbf6ebc90838de}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.}
      \field{month}{12}
      \field{title}{Spectral Networks and Locally Connected Networks on Graphs}
      \field{year}{2013}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1312.6203
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1312.6203
      \endverb
    \endentry
    \entry{Chaudhri2022}{article}{}
      \name{author}{11}{}{%
        {{un=0,uniquepart=base,hash=00e628cd44183c8b7774a1904199c7be}{%
           family={Chaudhri},
           familyi={C\bibinitperiod},
           given={Vinay\bibnamedelima K.},
           giveni={V\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=585e7d3a4cf1624fd916c6c2b8ac9136}{%
           family={Baru},
           familyi={B\bibinitperiod},
           given={Chaitanya},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f5e30c4b2e2d8dbb7f8bc9730646574b}{%
           family={Chittar},
           familyi={C\bibinitperiod},
           given={Naren},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3fb05cb08a8960aeecf7fa6c8a880f1c}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Xin\bibnamedelima Luna},
           giveni={X\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cbd2d0e3751337a87c4d64076368e0fb}{%
           family={Genesereth},
           familyi={G\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b03c1c11511fa3c1c6224a741f227be4}{%
           family={Hendler},
           familyi={H\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e0d31f5d176c659833a81c1fe3de7302}{%
           family={Kalyanpur},
           familyi={K\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2ef67f8a5050ae70229240f255d2ee45}{%
           family={Lenat},
           familyi={L\bibinitperiod},
           given={Douglas\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d2c4c78dacb7d5c0f840ed22b1fd0c8}{%
           family={Sequeda},
           familyi={S\bibinitperiod},
           given={Juan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e45ed1115b5773b6355eb03f326041a4}{%
           family={Vrandeƒçiƒá},
           familyi={V\bibinitperiod},
           given={Denny},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3bb8bbc6e6ba11f233f270fb5436546}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Kuansan},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{2}{%
        {John Wiley}%
        {Sons Inc}%
      }
      \strng{namehash}{a2245e3c101a2b5a283fdb646887dae3}
      \strng{fullhash}{e39e1ec6ff9d2e140bd29e75afbe1e60}
      \strng{bibnamehash}{e39e1ec6ff9d2e140bd29e75afbe1e60}
      \strng{authorbibnamehash}{e39e1ec6ff9d2e140bd29e75afbe1e60}
      \strng{authornamehash}{a2245e3c101a2b5a283fdb646887dae3}
      \strng{authorfullhash}{e39e1ec6ff9d2e140bd29e75afbe1e60}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge graphs (KGs) have emerged as a compelling abstraction for organizing the world's structured knowledge and for integrating information extracted from multiple data sources. They are also beginning to play a central role in representing information extracted by AI systems, and for improving the predictions of AI systems by giving them knowledge expressed in KGs as input. The goals of this article are to (a) introduce KGs and discuss important areas of application that have gained recent prominence; (b) situate KGs in the context of the prior work in AI; and (c) present a few contrasting perspectives that help in better understanding KGs in relation to related technologies.}
      \field{issn}{23719621}
      \field{issue}{1}
      \field{journaltitle}{AI Magazine}
      \field{month}{3}
      \field{title}{Knowledge graphs: Introduction, history, and perspectives}
      \field{volume}{43}
      \field{year}{2022}
      \field{pages}{17\bibrangedash 29}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1002/aaai.12033
      \endverb
    \endentry
    \entry{Chen2017}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=916b50b17e2bbb295f7b575d3da19425}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Jianfei},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7a47a154652ccc647a0e6adf1570095a}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=da5283630e781fb48ecf4859f54bca52}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Le},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7d822c997b2ed38e8e6392246eec3a29}
      \strng{fullhash}{d9dc4dc0b6fa3c1cd35cb9f2f0ddc817}
      \strng{bibnamehash}{d9dc4dc0b6fa3c1cd35cb9f2f0ddc817}
      \strng{authorbibnamehash}{d9dc4dc0b6fa3c1cd35cb9f2f0ddc817}
      \strng{authornamehash}{7d822c997b2ed38e8e6392246eec3a29}
      \strng{authorfullhash}{d9dc4dc0b6fa3c1cd35cb9f2f0ddc817}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.}
      \field{month}{10}
      \field{title}{Stochastic Training of Graph Convolutional Networks with Variance Reduction}
      \field{year}{2017}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1710.10568
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1710.10568
      \endverb
    \endentry
    \entry{Chiang2019}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=8e6b99db00c3729150cacdb24ff660f1}{%
           family={Chiang},
           familyi={C\bibinitperiod},
           given={Wei\bibnamedelima Lin},
           giveni={W\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2af761d9bef859ea29248418adce0dd7}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yang},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=40d065205716f92199ccec87c23fba98}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Xuanqing},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=02404a92b0be3f52ec5ac08e41c13445}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Samy},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1d9bec203d7a6cdf2651ad8a4cf990c6}{%
           family={Si},
           familyi={S\bibinitperiod},
           given={Si},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2c8ef4ae60ab2e95fc732266ac98756a}{%
           family={Hsieh},
           familyi={H\bibinitperiod},
           given={Cho\bibnamedelima Jui},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{0f714cd94b776c49f38d1cec7d148cdd}
      \strng{fullhash}{8de68df2d40358dc5d4a2ecd69436ecc}
      \strng{bibnamehash}{8de68df2d40358dc5d4a2ecd69436ecc}
      \strng{authorbibnamehash}{8de68df2d40358dc5d4a2ecd69436ecc}
      \strng{authornamehash}{0f714cd94b776c49f38d1cec7d148cdd}
      \strng{authorfullhash}{8de68df2d40358dc5d4a2ecd69436ecc}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy-using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16].}
      \field{isbn}{9781450362016}
      \field{journaltitle}{Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}
      \field{month}{7}
      \field{title}{Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks}
      \field{year}{2019}
      \field{pages}{257\bibrangedash 266}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/3292500.3330925
      \endverb
    \endentry
    \entry{Cui2020}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=c7f6a3d1688003e0a41e49e8ce85f01e}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Zhiyong},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a17760223fb7507665661e8878f4dec1}{%
           family={Henrickson},
           familyi={H\bibinitperiod},
           given={Kristian},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a10dc3fa6ad54be59cdc054e19c8e139}{%
           family={Ke},
           familyi={K\bibinitperiod},
           given={Ruimin},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=10823b6d64e8068836ff795955b9efb9}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yinhai},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{2}{%
        {Institute of Electrical}%
        {Electronics Engineers Inc.}%
      }
      \strng{namehash}{3a576ece3cad27540d9bee29c4ca78c5}
      \strng{fullhash}{9a3a7f75578fbc640fcff1dcf5ccc330}
      \strng{bibnamehash}{9a3a7f75578fbc640fcff1dcf5ccc330}
      \strng{authorbibnamehash}{9a3a7f75578fbc640fcff1dcf5ccc330}
      \strng{authornamehash}{3a576ece3cad27540d9bee29c4ca78c5}
      \strng{authorfullhash}{9a3a7f75578fbc640fcff1dcf5ccc330}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Traffic forecasting is a particularly challenging application of spatiotemporal forecasting, due to the time-varying traffic patterns and the complicated spatial dependencies on road networks. To address this challenge, we learn the traffic network as a graph and propose a novel deep learning framework, Traffic Graph Convolutional Long Short-Term Memory Neural Network (TGC-LSTM), to learn the interactions between roadways in the traffic network and forecast the network-wide traffic state. We define the traffic graph convolution based on the physical network topology. The relationship between the proposed traffic graph convolution and the spectral graph convolution is also discussed. An L1-norm on graph convolution weights and an L2-norm on graph convolution features are added to the model's loss function to enhance the interpretability of the proposed model. Experimental results show that the proposed model outperforms baseline methods on two real-world traffic state datasets. The visualization of the graph convolution weights indicates that the proposed framework can recognize the most influential road segments in real-world traffic networks.}
      \field{issn}{15580016}
      \field{issue}{11}
      \field{journaltitle}{IEEE Transactions on Intelligent Transportation Systems}
      \field{month}{11}
      \field{title}{Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting}
      \field{volume}{21}
      \field{year}{2020}
      \field{pages}{4883\bibrangedash 4894}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1109/TITS.2019.2950416
      \endverb
      \keyw{LSTM,Traffic forecasting,graph convolution,recurrent neural network,spatialoral}
    \endentry
    \entry{Cyganiak14RCA}{report}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=ff065518eacac535eec041c322c21896}{%
           family={Cyganiak},
           familyi={C\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d91ccb882c2defa0b7d5801d3486fe95}{%
           family={Lanthaler},
           familyi={L\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ad0aae768a1174aba9738b6d2719985b}{%
           family={Wood},
           familyi={W\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{1}{%
        {W3C}%
      }
      \strng{namehash}{239617b5251160922f9c0eb4e93d3c6d}
      \strng{fullhash}{5046a570263d636dd662bcf6bb4ba7e6}
      \strng{bibnamehash}{5046a570263d636dd662bcf6bb4ba7e6}
      \strng{authorbibnamehash}{5046a570263d636dd662bcf6bb4ba7e6}
      \strng{authornamehash}{239617b5251160922f9c0eb4e93d3c6d}
      \strng{authorfullhash}{5046a570263d636dd662bcf6bb4ba7e6}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{2}
      \field{title}{{RDF} 1.1 Concepts and Abstract Syntax}
      \field{type}{W3C Recommendation}
      \field{year}{2014}
      \verb{urlraw}
      \verb https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/
      \endverb
      \verb{url}
      \verb https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/
      \endverb
    \endentry
    \entry{Deborah2004}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=2f919e7c0a21f2f5eb2021e732e42711}{%
           family={Deborah},
           familyi={D\bibinitperiod},
           given={L.\bibnamedelimi McGuinness},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ee480b6d2fd2c627ffb033d94c9691af}{%
           family={Harmelen\bibnamedelima Frank},
           familyi={H\bibinitperiod\bibinitdelim F\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
      }
      \strng{namehash}{2978fba5ed082666175dce0d76bd5df0}
      \strng{fullhash}{2978fba5ed082666175dce0d76bd5df0}
      \strng{bibnamehash}{2978fba5ed082666175dce0d76bd5df0}
      \strng{authorbibnamehash}{2978fba5ed082666175dce0d76bd5df0}
      \strng{authornamehash}{2978fba5ed082666175dce0d76bd5df0}
      \strng{authorfullhash}{2978fba5ed082666175dce0d76bd5df0}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The OWL Web Ontology Language is designed for use by applications that need to process the content of information instead of just presenting information to humans. OWL facilitates greater machine interpretability of Web content than that supported by XML, RDF, and RDF Schema (RDF-S) by providing additional vocabulary along with a formal semantics. OWL has three increasingly-expressive sublanguages: OWL Lite, OWL DL, and OWL Full.}
      \field{title}{OWL Web Ontology Language Overview}
      \field{year}{2004}
      \verb{urlraw}
      \verb http://www.w3.org/TR/2003/PR-owl-features-20031215/
      \endverb
      \verb{url}
      \verb http://www.w3.org/TR/2003/PR-owl-features-20031215/
      \endverb
    \endentry
    \entry{Defferrard2016}{misc}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=1c45ec3985cdb526e1b18bf4041b4c22}{%
           family={Defferrard},
           familyi={D\bibinitperiod},
           given={Micha√´l},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=26bf36a5046daec764f03647500c3871}{%
           family={Bresson},
           familyi={B\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b4945596e71bcffbc9d1b4af0d5e16f1}{%
           family={Vandergheynst},
           familyi={V\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{08ab1a63b0c357f6c1df04bd4deec745}
      \strng{fullhash}{9c0e667960f917ece13a407278f995f6}
      \strng{bibnamehash}{9c0e667960f917ece13a407278f995f6}
      \strng{authorbibnamehash}{9c0e667960f917ece13a407278f995f6}
      \strng{authornamehash}{08ab1a63b0c357f6c1df04bd4deec745}
      \strng{authorfullhash}{9c0e667960f917ece13a407278f995f6}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.}
      \field{title}{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}
      \field{year}{2016}
      \verb{urlraw}
      \verb https://github.com/mdeff/cnn_graph
      \endverb
      \verb{url}
      \verb https://github.com/mdeff/cnn_graph
      \endverb
    \endentry
    \entry{Fan2019}{article}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=c962fa5377a09410d9645db6e5704558}{%
           family={Fan},
           familyi={F\bibinitperiod},
           given={Wenqi},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9495c8a5a2c9489e8af928b6f0eed6b}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Yao},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=16a77262afb7cc9b0229c21f168bd098}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Qing},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bcd84142f4f61ad0c884a49e5688c6fa}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6c876310822725ae4936f8c79aedc716}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=854278958e1021c53ca312cc8316e2a6}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Jiliang},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=866fc31d9db55acbc0a55c2b3c929414}{%
           family={Yin},
           familyi={Y\bibinitperiod},
           given={Dawei},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery, Inc}%
      }
      \strng{namehash}{eb25007ac5b5dcfb3bbc997db67f5ab4}
      \strng{fullhash}{9a47c531d96e3f331a21b1573bd1cefe}
      \strng{bibnamehash}{9a47c531d96e3f331a21b1573bd1cefe}
      \strng{authorbibnamehash}{9a47c531d96e3f331a21b1573bd1cefe}
      \strng{authornamehash}{eb25007ac5b5dcfb3bbc997db67f5ab4}
      \strng{authorfullhash}{9a47c531d96e3f331a21b1573bd1cefe}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.}
      \field{isbn}{9781450366748}
      \field{journaltitle}{The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019}
      \field{month}{5}
      \field{title}{Graph neural networks for social recommendation}
      \field{year}{2019}
      \field{pages}{417\bibrangedash 426}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/3308558.3313488
      \endverb
      \keyw{Graph Neural Networks,Neural Networks,Recommender Systems,Social Network,Social Recommendation}
    \endentry
    \entry{Fernandez2011}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=462d87942086f19e5053c4acdda8472e}{%
           family={Fern√°ndez},
           familyi={F\bibinitperiod},
           given={Miriam},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=193ff56861fe0a1142780938dc3ac715}{%
           family={Cantador},
           familyi={C\bibinitperiod},
           given={Iv√°n},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ac71a73da3b0bd23a865ce0bf066f7ef}{%
           family={L√≥pez},
           familyi={L\bibinitperiod},
           given={Vanesa},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a61d6e33971a2a89f93a2b1113955d61}{%
           family={Vallet},
           familyi={V\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=27f68737cb3395ff156325e510de28c0}{%
           family={Castells},
           familyi={C\bibinitperiod},
           given={Pablo},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=32f2e25e4d849ddcc3a9cc11337f4ad6}{%
           family={Motta},
           familyi={M\bibinitperiod},
           given={Enrico},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{59327ba5d5f512c04b499559b6a0d47d}
      \strng{fullhash}{983ab8d5845646b687c196328a6134cb}
      \strng{bibnamehash}{983ab8d5845646b687c196328a6134cb}
      \strng{authorbibnamehash}{983ab8d5845646b687c196328a6134cb}
      \strng{authornamehash}{59327ba5d5f512c04b499559b6a0d47d}
      \strng{authorfullhash}{983ab8d5845646b687c196328a6134cb}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Currently, techniques for content description and query processing in Information Retrieval (IR) are based on keywords, and therefore provide limited capabilities to capture the conceptualizations associated with user needs and contents. Aiming to solve the limitations of keyword-based models, the idea of conceptual search, understood as searching by meanings rather than literal strings, has been the focus of a wide body of research in the IR field. More recently, it has been used as a prototypical scenario (or even envisioned as a potential "killer app") in the Semantic Web (SW) vision, since its emergence in the late nineties. However, current approaches to semantic search developed in the SW area have not yet taken full advantage of the acquired knowledge, accumulated experience, and technological sophistication achieved through several decades of work in the IR field. Starting from this position, this work investigates the definition of an ontology-based IR model, oriented to the exploitation of domain Knowledge Bases to support semantic search capabilities in large document repositories, stressing on the one hand the use of fully fledged ontologies in the semantic-based perspective, and on the other hand the consideration of unstructured content as the target search space. The major contribution of this work is an innovative, comprehensive semantic search model, which extends the classic IR model, addresses the challenges of the massive and heterogeneous Web environment, and integrates the benefits of both keyword and semantic-based search. Additional contributions include: an innovative rank fusion technique that minimizes the undesired effects of knowledge sparseness on the yet juvenile SW, and the creation of a large-scale evaluation benchmark, based on TREC IR evaluation standards, which allows a rigorous comparison between IR and SW approaches. Conducted experiments show that our semantic search model obtained comparable and better performance results (in terms of MAP and P@10 values) than the best TREC automatic system. ¬© 2010 Elsevier B.V. All rights reserved.}
      \field{issn}{15708268}
      \field{issue}{4}
      \field{journaltitle}{Journal of Web Semantics}
      \field{month}{12}
      \field{title}{Semantically enhanced Information Retrieval: An ontology-based approach}
      \field{volume}{9}
      \field{year}{2011}
      \field{pages}{434\bibrangedash 452}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1016/j.websem.2010.11.003
      \endverb
      \keyw{Information Retrieval,Semantic Web,Semantic search}
    \endentry
    \entry{Fout2017}{misc}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=5679b1b6b017b7b38e193df4c906e448}{%
           family={Fout},
           familyi={F\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e81d3835d88a7cb87bf3f0d7e32d15db}{%
           family={Byrd},
           familyi={B\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d7a817654a00d07f28a89d90fad42b6f}{%
           family={Shariat},
           familyi={S\bibinitperiod},
           given={Basir},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b7f90707488abd2a2b8ef897e7232b65}{%
           family={Ben-Hur},
           familyi={B\bibinithyphendelim H\bibinitperiod},
           given={Asa},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{998816d276a75efb02b704124b4b17f1}
      \strng{fullhash}{c88e36f9414f0c801a671fd3b41dc599}
      \strng{bibnamehash}{c88e36f9414f0c801a671fd3b41dc599}
      \strng{authorbibnamehash}{c88e36f9414f0c801a671fd3b41dc599}
      \strng{authornamehash}{998816d276a75efb02b704124b4b17f1}
      \strng{authorfullhash}{c88e36f9414f0c801a671fd3b41dc599}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.}
      \field{title}{Protein Interface Prediction using Graph Convolutional Networks}
      \field{year}{2017}
    \endentry
    \entry{Francis2018}{article}{}
      \name{author}{10}{}{%
        {{un=0,uniquepart=base,hash=fc9931b30bb3393a9e60514e90139510}{%
           family={Francis},
           familyi={F\bibinitperiod},
           given={Nadime},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86bf31b4f157017436ed9fb6023ec8ca}{%
           family={Green},
           familyi={G\bibinitperiod},
           given={Alastair},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fc5fe1a24439ad6b97eae55ca70033ad}{%
           family={Guagliardo},
           familyi={G\bibinitperiod},
           given={Paolo},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=931db7d26f75777de801bc1858e3ebae}{%
           family={Libkin},
           familyi={L\bibinitperiod},
           given={Leonid},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2448e765907e336b42503ef5181866f1}{%
           family={Lindaaker},
           familyi={L\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8f2de4fad67143c7e42ca23b5081e234}{%
           family={Marsault},
           familyi={M\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=08f838be08287bafe82e8baa6c98e89d}{%
           family={Plantikow},
           familyi={P\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3dfaf4ef8c4f1fd20666ec4c619ba5af}{%
           family={Rydberg},
           familyi={R\bibinitperiod},
           given={Mats},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3eae824625f74f671ce3b090ba521c6c}{%
           family={Selmer},
           familyi={S\bibinitperiod},
           given={Petra},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85e616cabb1eb74ff4903753889e5e92}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Andr√©s},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{53e7edfd3561bb5b26caefe7f3a584df}
      \strng{fullhash}{00a2ad39f3b5c57ac3676442081683dc}
      \strng{bibnamehash}{00a2ad39f3b5c57ac3676442081683dc}
      \strng{authorbibnamehash}{00a2ad39f3b5c57ac3676442081683dc}
      \strng{authornamehash}{53e7edfd3561bb5b26caefe7f3a584df}
      \strng{authorfullhash}{00a2ad39f3b5c57ac3676442081683dc}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Cypher property graph query language is an evolving language, originally designed and implemented as part of the Neo4j graph database, and it is currently used by several commercial database products and researchers. We describe Cypher 9, which is the first version of the language governed by the openCypher Implementers Group. We first introduce the language by example, and describe its uses in industry. We then provide a formal semantic definition of the core read-query features of Cypher, including its variant of the property graph data model, and its "ASCII Art" graph pattern matching mechanism for expressing subgraphs of interest to an application. We compare the features of Cypher to other property graph query languages, and describe extensions, at an advanced stage of development, which will form part of Cypher 10, turning the language into a compositional language which supports graph projections and multiple named graphs.}
      \field{isbn}{9781450317436}
      \field{issn}{07308078}
      \field{journaltitle}{Proceedings of the ACM SIGMOD International Conference on Management of Data}
      \field{month}{5}
      \field{title}{Cypher: An evolving query language for property graphs}
      \field{year}{2018}
      \field{pages}{1433\bibrangedash 1445}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1145/3183713.3190657
      \endverb
    \endentry
    \entry{Antoniou2008}{book}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=703f01bb613f0f2dd501efab06d3c890}{%
           family={{G. (Grigoris) Antoniou and Frank. Van Harmelen}},
           familyi={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{703f01bb613f0f2dd501efab06d3c890}
      \strng{fullhash}{703f01bb613f0f2dd501efab06d3c890}
      \strng{bibnamehash}{703f01bb613f0f2dd501efab06d3c890}
      \strng{authorbibnamehash}{703f01bb613f0f2dd501efab06d3c890}
      \strng{authornamehash}{703f01bb613f0f2dd501efab06d3c890}
      \strng{authorfullhash}{703f01bb613f0f2dd501efab06d3c890}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{2nd ed. The substantially updated second edition of a widely used guide to the key ideas, languages, and technologies of the Semantic Web, featuring additional coverage of new application areas, new tools, and other recent developments. Brief Contents -- Contents -- List of Figures -- Series Foreword -- Preface -- 1 The Semantic Web Vision -- 2 Structured Web Documents: XML -- 3 Describing Web Resources: RDF -- 4 Web Ontology Language: OWL -- 5 Logic and Inference: Rules -- 6 Applications -- 7 Ontology Engineering -- 8 Conclusion and Outlook -- A Abstract OWL Syntax -- Index}
      \field{isbn}{9780262012423}
      \field{title}{A semantic Web primer}
      \field{year}{2008}
      \field{pages}{264}
      \range{pages}{1}
    \endentry
    \entry{Gilmer2017}{misc}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=4f550339f0337905aa634f39e1ba4833}{%
           family={Gilmer},
           familyi={G\bibinitperiod},
           given={Justin},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=aa3475bbb938eec3f9f0d7c8d44f1e86}{%
           family={Schoenholz},
           familyi={S\bibinitperiod},
           given={Samuel\bibnamedelima S},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e9632c2c29a574cd09a5a7503be60435}{%
           family={Riley},
           familyi={R\bibinitperiod},
           given={Patrick\bibnamedelima F},
           giveni={P\bibinitperiod\bibinitdelim F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b2367e57c17225a47853346440e87b35}{%
           family={Dahl},
           familyi={D\bibinitperiod},
           given={George\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c654290e16b9e71825428d767650ff2a}
      \strng{fullhash}{a97dbeb15b563be460fc056e7a076a4b}
      \strng{bibnamehash}{a97dbeb15b563be460fc056e7a076a4b}
      \strng{authorbibnamehash}{a97dbeb15b563be460fc056e7a076a4b}
      \strng{authornamehash}{c654290e16b9e71825428d767650ff2a}
      \strng{authorfullhash}{a97dbeb15b563be460fc056e7a076a4b}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery , and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper , we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
      \field{title}{Neural Message Passing for Quantum Chemistry}
      \field{year}{2017}
    \endentry
    \entry{Gori2005}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=d25d4f83ae540b2b728a5423e9fbe85b}{%
           family={Gori},
           familyi={G\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b56d5f9b0efe692c5e81a88da3a2f2b7}{%
           family={Monfardini},
           familyi={M\bibinitperiod},
           given={Gabriele},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=447de7983619243dc5bf3af69d609b20}{%
           family={Scarselli},
           familyi={S\bibinitperiod},
           given={Franco},
           giveni={F\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0c3e1f358b87e3ab79622e6504ebcaaf}
      \strng{fullhash}{9c8447cad4e780034e98b80b7b341453}
      \strng{bibnamehash}{9c8447cad4e780034e98b80b7b341453}
      \strng{authorbibnamehash}{9c8447cad4e780034e98b80b7b341453}
      \strng{authornamehash}{0c3e1f358b87e3ab79622e6504ebcaaf}
      \strng{authorfullhash}{9c8447cad4e780034e98b80b7b341453}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In several applications the information is naturally represented by graphs. Traditional approaches cope with graphi-cal data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.}
      \field{isbn}{0-7803-9048-2}
      \field{issn}{2161-4407}
      \field{journaltitle}{IEEE International Joint Conference on Neural Networks}
      \field{title}{A New Model for Learning in Graph Domains}
      \field{volume}{5}
      \field{year}{2005}
      \verb{doi}
      \verb 10.1109/IJCNN.2005.1555942
      \endverb
      \keyw{Keywords}
    \endentry
    \entry{Grover2016}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=0e7ecffac4441691ff61e0b68d89cd33}{%
           family={Grover},
           familyi={G\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{f78ae7abf2608b5507c89d73eca50af5}
      \strng{fullhash}{f78ae7abf2608b5507c89d73eca50af5}
      \strng{bibnamehash}{f78ae7abf2608b5507c89d73eca50af5}
      \strng{authorbibnamehash}{f78ae7abf2608b5507c89d73eca50af5}
      \strng{authornamehash}{f78ae7abf2608b5507c89d73eca50af5}
      \strng{authorfullhash}{f78ae7abf2608b5507c89d73eca50af5}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9781450342322}
      \field{issn}{2154-817X}
      \field{journaltitle}{Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}
      \field{month}{8}
      \field{title}{Node2vec: Scalable feature learning for networks}
      \field{volume}{13-17-August-2016}
      \field{year}{2016}
      \field{pages}{855\bibrangedash 864}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/2939672.2939754
      \endverb
      \keyw{Feature learning,Graph representations,Information networks,Node embeddings}
    \endentry
    \entry{Hamilton2017}{misc}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=2abce6c40b4cc1a3d89f175883b24536}{%
           family={Hamilton},
           familyi={H\bibinitperiod},
           given={William\bibnamedelima L},
           giveni={W\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b37c8aefa150635663b2a6525952a7a}{%
           family={Ying},
           familyi={Y\bibinitperiod},
           given={Rex},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{272631a74039af14ab15f43cfaad8c7f}
      \strng{fullhash}{f818fe3ba25dc2399831a69d9b62c7f0}
      \strng{bibnamehash}{f818fe3ba25dc2399831a69d9b62c7f0}
      \strng{authorbibnamehash}{f818fe3ba25dc2399831a69d9b62c7f0}
      \strng{authornamehash}{272631a74039af14ab15f43cfaad8c7f}
      \strng{authorfullhash}{f818fe3ba25dc2399831a69d9b62c7f0}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.}
      \field{title}{Inductive Representation Learning on Large Graphs}
      \field{year}{2017}
    \endentry
    \entry{Hogan2021}{article}{}
      \name{author}{18}{}{%
        {{un=0,uniquepart=base,hash=56f2d88e83f41f6e1f0d9a0197c4c7eb}{%
           family={Hogan},
           familyi={H\bibinitperiod},
           given={Aidan},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=019293949e6289df4e3b751f3f9ecf4e}{%
           family={Blomqvist},
           familyi={B\bibinitperiod},
           given={Eva},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ac9f2c71e19d14efaa82ad1908d35843}{%
           family={Cochez},
           familyi={C\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b900f8333251448c0db95783a03e42d0}{%
           family={D'Amato},
           familyi={D\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=42945c403f20700ebbebb90b8531c982}{%
           family={Melo},
           familyi={M\bibinitperiod},
           given={Gerard\bibnamedelima De},
           giveni={G\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5feae03b05f14f0a5fd1f3e07badd8e6}{%
           family={Gutierrez},
           familyi={G\bibinitperiod},
           given={Claudio},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5ddbf3af24e3802f8842794c8c86b60c}{%
           family={Kirrane},
           familyi={K\bibinitperiod},
           given={Sabrina},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=75f24fe7a15315f35934fc2dfdbc6aa2}{%
           family={Gayo},
           familyi={G\bibinitperiod},
           given={Jos√©\bibnamedelimb Emilio\bibnamedelima Labra},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6fe24dce481a67e247581d4b20435709}{%
           family={Navigli},
           familyi={N\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9aabe7235b5cda4ebcca85f21cf40b60}{%
           family={Neumaier},
           familyi={N\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b4ea58f1dc4bfe0f1f28fd990982868e}{%
           family={Ngomo},
           familyi={N\bibinitperiod},
           given={Axel\bibnamedelimb Cyrille\bibnamedelima Ngonga},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a2be0ec850b04b1f5e9fe3aca6bf5226}{%
           family={Polleres},
           familyi={P\bibinitperiod},
           given={Axel},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=95ae583344f22f1d58dae51c33b60901}{%
           family={Rashid},
           familyi={R\bibinitperiod},
           given={Sabbir\bibnamedelima M.},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1bff37423f112cd817f8975c889c0791}{%
           family={Rula},
           familyi={R\bibinitperiod},
           given={Anisa},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9c1975136125fdac517346a463335503}{%
           family={Schmelzeisen},
           familyi={S\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d2c4c78dacb7d5c0f840ed22b1fd0c8}{%
           family={Sequeda},
           familyi={S\bibinitperiod},
           given={Juan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3489f3873d85937d1977e3a21e5e76a7}{%
           family={Staab},
           familyi={S\bibinitperiod},
           given={Steffen},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2472f33c293d18b30767d8cd170089fd}{%
           family={Zimmermann},
           familyi={Z\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{fullhash}{dfe027f701b4f1db6d1ca5782d0375a5}
      \strng{bibnamehash}{dfe027f701b4f1db6d1ca5782d0375a5}
      \strng{authorbibnamehash}{dfe027f701b4f1db6d1ca5782d0375a5}
      \strng{authornamehash}{d5ce211d54ee18055922d1ccf1a63ea0}
      \strng{authorfullhash}{dfe027f701b4f1db6d1ca5782d0375a5}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.}
      \field{issn}{15577341}
      \field{issue}{4}
      \field{journaltitle}{ACM Computing Surveys}
      \field{month}{7}
      \field{title}{Knowledge graphs}
      \field{volume}{54}
      \field{year}{2021}
      \verb{doi}
      \verb 10.1145/3447772
      \endverb
      \keyw{Embeddings,Graph algorithms,Graph databases,Graph neural networks,Graph query languages,Knowledge graphs,Ontologies,Rule mining,Shapes}
    \endentry
    \entry{Jin2018}{misc}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=1b4ff42a6b4aab3a2d6ca92ae02b805a}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Wengong},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=de391703fb0c2fdf733b3a0095bc32ce}{%
           family={Barzilay},
           familyi={B\bibinitperiod},
           given={Regina},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=521085652a07e67bdff5323e2febc138}{%
           family={Jaakkola},
           familyi={J\bibinitperiod},
           given={Tommi},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{783d51c5994de42803c489579f1c7d93}
      \strng{fullhash}{079298939281158af5eff8edbe3d0e02}
      \strng{bibnamehash}{079298939281158af5eff8edbe3d0e02}
      \strng{authorbibnamehash}{079298939281158af5eff8edbe3d0e02}
      \strng{authornamehash}{783d51c5994de42803c489579f1c7d93}
      \strng{authorfullhash}{079298939281158af5eff8edbe3d0e02}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model out-performs previous state-of-the-art baselines by a significant margin.}
      \field{title}{Junction Tree Variational Autoencoder for Molecular Graph Generation}
      \field{year}{2018}
    \endentry
    \entry{Kapanipathi2020}{article}{}
      \name{author}{30}{}{%
        {{un=0,uniquepart=base,hash=39e52e141a4c6b3ccbae17613c42d305}{%
           family={Kapanipathi},
           familyi={K\bibinitperiod},
           given={Pavan},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6579e6eafeb754dcd11b86a3895310b6}{%
           family={Abdelaziz},
           familyi={A\bibinitperiod},
           given={Ibrahim},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bfcc9a153b673e0d8a01e654b89b29aa}{%
           family={Ravishankar},
           familyi={R\bibinitperiod},
           given={Srinivas},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fcdf7cf3f3cfe87d0f08f8f4d98b1137}{%
           family={Roukos},
           familyi={R\bibinitperiod},
           given={Salim},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a263cec8e9c2448c55c8a3ccad6ea03d}{%
           family={Gray},
           familyi={G\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1623273efad3aca25e2c9cb374ad2e2a}{%
           family={Astudillo},
           familyi={A\bibinitperiod},
           given={Ramon},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=561a89f49dcedc04b1b459a57d087b7a}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Maria},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ca6353e6404f26e7a979f8d7433ee9fa}{%
           family={Cornelio},
           familyi={C\bibinitperiod},
           given={Cristina},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=288776bbe18f59a6c19647c1fa16a7c1}{%
           family={Dana},
           familyi={D\bibinitperiod},
           given={Saswati},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9a26b52e478d6baff0da0f3f34302804}{%
           family={Fokoue},
           familyi={F\bibinitperiod},
           given={Achille},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4a59d3b9430b2ffb86741a36f3a9183e}{%
           family={Garg},
           familyi={G\bibinitperiod},
           given={Dinesh},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f9ecaa7cc8a1dbcf2d0816e34fd8bc07}{%
           family={Gliozzo},
           familyi={G\bibinitperiod},
           given={Alfio},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=92ae5867927b61661c3d3bdaf322d1c7}{%
           family={Gurajada},
           familyi={G\bibinitperiod},
           given={Sairam},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e84bfd3ad1f52b77be27a7705392fc43}{%
           family={Karanam},
           familyi={K\bibinitperiod},
           given={Hima},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e24b06310e9af51910d4945ccef604fc}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Naweed},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ebeaa071daabcd836fe8ce294483da48}{%
           family={Khandelwal},
           familyi={K\bibinitperiod},
           given={Dinesh},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a470ff47a10618cc3a8a28e105ae17dc}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Young-Suk},
           giveni={Y\bibinithyphendelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8820673f62adf33c74365bc576fda374}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yunyao},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6c3f0afd4ef0282df8ae243b0ef07974}{%
           family={Luus},
           familyi={L\bibinitperiod},
           given={Francois},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=de0d4a0a64efb01f7939d4758b4a88b5}{%
           family={Makondo},
           familyi={M\bibinitperiod},
           given={Ndivhuwo},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f90a6b92dd25c18b3e8c87cc918532f9}{%
           family={Mihindukulasooriya},
           familyi={M\bibinitperiod},
           given={Nandana},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=47018ed878c62b243fe67d1229865f99}{%
           family={Naseem},
           familyi={N\bibinitperiod},
           given={Tahira},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8726fd73a3bc365cf19bb252ded963ed}{%
           family={Neelam},
           familyi={N\bibinitperiod},
           given={Sumit},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cca4707fa68c79d531c33d107a5d031e}{%
           family={Popa},
           familyi={P\bibinitperiod},
           given={Lucian},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=53eb96aa8dbc8d49bc3c789610fa860d}{%
           family={Reddy},
           familyi={R\bibinitperiod},
           given={Revanth},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f858f7e911e129ebb7005af66081dd02}{%
           family={Riegel},
           familyi={R\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=56e7966cc31b61f6e1703221e9a04f82}{%
           family={Rossiello},
           familyi={R\bibinitperiod},
           given={Gaetano},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6028f0a7359c3b953575996d43ccf81a}{%
           family={Sharma},
           familyi={S\bibinitperiod},
           given={Udit},
           giveni={U\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d6c7d20f3a099b36a8238d696d48cd3}{%
           family={Bhargav},
           familyi={B\bibinitperiod},
           given={G\bibnamedelima P\bibnamedelima Shrivatsa},
           giveni={G\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=895459f795b2530f8583442ac07ff7c5}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Mo},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{bb6cd5e6c5a7d119850d53d4c29097d0}
      \strng{fullhash}{1614f91eb03a07ee4dc5025e38221326}
      \strng{bibnamehash}{85e5f836bbca797e1b0f205d5ee1864a}
      \strng{authorbibnamehash}{85e5f836bbca797e1b0f205d5ee1864a}
      \strng{authornamehash}{bb6cd5e6c5a7d119850d53d4c29097d0}
      \strng{authorfullhash}{1614f91eb03a07ee4dc5025e38221326}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge base question answering (KBQA)is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity andrelationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems.}
      \field{month}{12}
      \field{title}{Leveraging Abstract Meaning Representation for Knowledge Base Question Answering}
      \field{year}{2020}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2012.01707
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2012.01707
      \endverb
    \endentry
    \entry{Kejriwal2022}{misc}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=8a9721a2ee408f62c28c8648a3c89490}{%
           family={Kejriwal},
           familyi={K\bibinitperiod},
           given={Mayank},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {MDPI}%
      }
      \strng{namehash}{8a9721a2ee408f62c28c8648a3c89490}
      \strng{fullhash}{8a9721a2ee408f62c28c8648a3c89490}
      \strng{bibnamehash}{8a9721a2ee408f62c28c8648a3c89490}
      \strng{authorbibnamehash}{8a9721a2ee408f62c28c8648a3c89490}
      \strng{authornamehash}{8a9721a2ee408f62c28c8648a3c89490}
      \strng{authorfullhash}{8a9721a2ee408f62c28c8648a3c89490}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge graphs (KGs) have rapidly emerged as an important area in AI over the last ten years. Building on a storied tradition of graphs in the AI community, a KG may be simply defined as a directed, labeled, multi-relational graph with some form of semantics. In part, this has been fueled by increased publication of structured datasets on the Web, and well-publicized successes of large-scale projects such as the Google Knowledge Graph and the Amazon Product Graph. However, another factor that is less discussed, but which has been equally instrumental in the success of KGs, is the cross-disciplinary nature of academic KG research. Arguably, because of the diversity of this research, a synthesis of how different KG research strands all tie together could serve a useful role in enabling more ‚Äòmoonshot' research and large-scale collaborations. This review of the KG research landscape attempts to provide such a synthesis by first showing what the major strands of research are, and how those strands map to different communities, such as Natural Language Processing, Databases and Semantic Web. A unified framework is suggested in which to view the distinct, but overlapping, foci of KG research within these communities.}
      \field{issn}{20782489}
      \field{issue}{4}
      \field{journaltitle}{Information (Switzerland)}
      \field{month}{4}
      \field{title}{Knowledge Graphs: A Practical Review of the Research Landscape}
      \field{volume}{13}
      \field{year}{2022}
      \verb{doi}
      \verb 10.3390/info13040161
      \endverb
      \keyw{applications,data mining,graph databases,knowledge graphs,knowledge representation,natural language processing,semantic web}
    \endentry
    \entry{Khemani2024}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=065a77f23b1c3832aeacb1269479591f}{%
           family={Khemani},
           familyi={K\bibinitperiod},
           given={Bharti},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f7bb53bc184b6b3d5f205081ec5ddb97}{%
           family={Patil},
           familyi={P\bibinitperiod},
           given={Shruti},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=be34a381d3b31caac0e4a8150da4a303}{%
           family={Kotecha},
           familyi={K\bibinitperiod},
           given={Ketan},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=623ec3ea2244bcc12d0556928295ce14}{%
           family={Tanwar},
           familyi={T\bibinitperiod},
           given={Sudeep},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Springer Nature}%
      }
      \strng{namehash}{5023732984d5a319abc04b921530ccc7}
      \strng{fullhash}{f06823153f5afeb70634609bb067ad03}
      \strng{bibnamehash}{f06823153f5afeb70634609bb067ad03}
      \strng{authorbibnamehash}{f06823153f5afeb70634609bb067ad03}
      \strng{authornamehash}{5023732984d5a319abc04b921530ccc7}
      \strng{authorfullhash}{f06823153f5afeb70634609bb067ad03}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning has seen significant growth recently and is now applied to a wide range of conventional use cases, including graphs. Graph data provides relational information between elements and is a standard data format for various machine learning and deep learning tasks. Models that can learn from such inputs are essential for working with graph data effectively. This paper identifies nodes and edges within specific applications, such as text, entities, and relations, to create graph structures. Different applications may require various graph neural network (GNN) models. GNNs facilitate the exchange of information between nodes in a graph, enabling them to understand dependencies within the nodes and edges. The paper delves into specific GNN models like graph convolution networks (GCNs), GraphSAGE, and graph attention networks (GATs), which are widely used in various applications today. It also discusses the message-passing mechanism employed by GNN models and examines the strengths and limitations of these models in different domains. Furthermore, the paper explores the diverse applications of GNNs, the datasets commonly used with them, and the Python libraries that support GNN models. It offers an extensive overview of the landscape of GNN research and its practical implementations.}
      \field{issn}{21961115}
      \field{issue}{1}
      \field{journaltitle}{Journal of Big Data}
      \field{month}{12}
      \field{title}{A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions}
      \field{volume}{11}
      \field{year}{2024}
      \verb{doi}
      \verb 10.1186/s40537-023-00876-4
      \endverb
      \keyw{Graph Attention Networks (GAT),Graph Convolution Network (GCN),Graph Neural Network (GNN),GraphSAGE,Message Passing Mechanism,Natural Language Processing (NLP)}
    \endentry
    \entry{Kipf2017}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=26d8bcbda6afaf96334bf47c8fb88a75}{%
           family={Kipf},
           familyi={K\bibinitperiod},
           given={Thomas\bibnamedelima N.},
           giveni={T\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=53d2880ad8047b61cdae2c6b2803e763}{%
           family={Welling},
           familyi={W\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{fullhash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{bibnamehash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{authorbibnamehash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{authornamehash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \strng{authorfullhash}{e04e5808dc0d0ec733212b81e1f8c78a}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.}
      \field{month}{9}
      \field{title}{Semi-Supervised Classification with Graph Convolutional Networks}
      \field{year}{2017}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1609.02907
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1609.02907
      \endverb
    \endentry
    \entry{Lee2018}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=9a27056f3867b1e2c2338abbc1f77c70}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={John\bibnamedelima Boaz},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e7c042e3ae9e4143e9960410c1aad008}{%
           family={Rossi},
           familyi={R\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1387c71d325f61dba70248c1c35725cb}{%
           family={Kong},
           familyi={K\bibinitperiod},
           given={Xiangnan},
           giveni={X\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{a1daa15e1b0d9d856909c40dfac44188}
      \strng{fullhash}{e2d6c3b50f26b3a2c72d96b91422fbd8}
      \strng{bibnamehash}{e2d6c3b50f26b3a2c72d96b91422fbd8}
      \strng{authorbibnamehash}{e2d6c3b50f26b3a2c72d96b91422fbd8}
      \strng{authornamehash}{a1daa15e1b0d9d856909c40dfac44188}
      \strng{authorfullhash}{e2d6c3b50f26b3a2c72d96b91422fbd8}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph classification is a problem with practical applications in many different domains. To solve this problem, one usually calculates certain graph statistics (i.e., graph features) that help discriminate between graphs of different classes. When calculating such features, most existing approaches process the entire graph. In a graphlet-based approach, for instance, the entire graph is processed to get the total count of different graphlets or subgraphs. In many real-world applications, however, graphs can be noisy with discriminative patterns confined to certain regions in the graph only. In this work, we study the problem of attention-based graph classification. The use of attention allows us to focus on small but informative parts of the graph, avoiding noise in the rest of the graph. We present a novel RNN model, called the Graph Attention Model (GAM), that processes only a portion of the graph by adaptively selecting a sequence of ‚Äúinformative‚Äù nodes. Experimental results on multiple real-world datasets show that the proposed method is competitive against various well-known methods in graph classification even though our method is limited to only a portion of the graph.}
      \field{isbn}{9781450355520}
      \field{journaltitle}{Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}
      \field{month}{7}
      \field{title}{Graph classification using structural attention}
      \field{year}{2018}
      \field{pages}{1666\bibrangedash 1674}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1145/3219819.3219980
      \endverb
      \keyw{Attentional processing,Deep learning,Graph mining,Reinforcement learning}
    \endentry
    \entry{Li2018}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=6606760bf79f45a1ee77fdc7eee90a5d}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Qimai},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fcde603e999f79cc0a0d6a450470b268}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Zhichao},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4f6f455b13d5d01b3c5b5505666199d2}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Xiao-Ming},
           giveni={X\bibinithyphendelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{1bbbe36d16f7acbb3d82a4d128682250}
      \strng{fullhash}{a877374b93a5895ee41f997edf776f8d}
      \strng{bibnamehash}{a877374b93a5895ee41f997edf776f8d}
      \strng{authorbibnamehash}{a877374b93a5895ee41f997edf776f8d}
      \strng{authornamehash}{1bbbe36d16f7acbb3d82a4d128682250}
      \strng{authorfullhash}{a877374b93a5895ee41f997edf776f8d}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires a considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.}
      \field{month}{1}
      \field{title}{Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning}
      \field{year}{2018}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1801.07606
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1801.07606
      \endverb
    \endentry
    \entry{Lin2015}{misc}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=e2994df0daa7033ced5356a51eed0321}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Yankai},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5b9a54b9d83697231299711961622ab8}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhiyuan},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=faa6f5041a22004ca0dd547c03476790}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Maosong},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=621013bf54d3546375ad71f17f020b5b}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Yang},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ece2de24c4f96348551e3079ead1fcbe}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Xuan},
           giveni={X\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f8ea2ca3c913483883f66e2f1f82347a}
      \strng{fullhash}{571cee062bd130e98a492da5a6abd12c}
      \strng{bibnamehash}{571cee062bd130e98a492da5a6abd12c}
      \strng{authorbibnamehash}{571cee062bd130e98a492da5a6abd12c}
      \strng{authornamehash}{f8ea2ca3c913483883f66e2f1f82347a}
      \strng{authorfullhash}{571cee062bd130e98a492da5a6abd12c}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments , we evaluate our models on three tasks including link prediction, triple classification and rela-tional fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.}
      \field{title}{Learning Entity and Relation Embeddings for Knowledge Graph Completion}
      \field{year}{2015}
      \verb{urlraw}
      \verb www.aaai.org
      \endverb
      \verb{url}
      \verb www.aaai.org
      \endverb
      \keyw{NLP and Knowledge Representation Track}
    \endentry
    \entry{manic2016building}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=cf84f0e168933e459a4dece1d450b65a}{%
           family={Manic},
           familyi={M\bibinitperiod},
           given={Milos},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0ba88d72ef2f94595e31e1ea56666405}{%
           family={Wijayasekara},
           familyi={W\bibinitperiod},
           given={Dumidu},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f8a86a36126958584ead762bae2b189e}{%
           family={Amarasinghe},
           familyi={A\bibinitperiod},
           given={Kasun},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ee7aeb8b3252ad39cafc59d245226f79}{%
           family={Rodriguez-Andina},
           familyi={R\bibinithyphendelim A\bibinitperiod},
           given={Juan\bibnamedelima J},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{f3383225913a7fd15b04ee56ccb6a80d}
      \strng{fullhash}{02c765ad85c03829d1009156ecaaf05c}
      \strng{bibnamehash}{02c765ad85c03829d1009156ecaaf05c}
      \strng{authorbibnamehash}{02c765ad85c03829d1009156ecaaf05c}
      \strng{authornamehash}{f3383225913a7fd15b04ee56ccb6a80d}
      \strng{authorfullhash}{02c765ad85c03829d1009156ecaaf05c}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{IEEE Industrial Electronics Magazine}
      \field{number}{1}
      \field{title}{Building energy management systems: The age of intelligent and adaptive buildings}
      \field{volume}{10}
      \field{year}{2016}
      \field{pages}{25\bibrangedash 39}
      \range{pages}{15}
    \endentry
    \entry{Monti2016}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=8f0d23e8869cd90a91c7a2d49ae85c0e}{%
           family={Monti},
           familyi={M\bibinitperiod},
           given={Federico},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b635105d5c572604739cdc9a9c13b5e9}{%
           family={Boscaini},
           familyi={B\bibinitperiod},
           given={Davide},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=296aa51baf6b5dce74c13806342b5bbe}{%
           family={Masci},
           familyi={M\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=67340328e52553a972aa6fded69d1699}{%
           family={Rodol√†},
           familyi={R\bibinitperiod},
           given={Emanuele},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f9976a69550d434ce587ee097b58ff26}{%
           family={Svoboda},
           familyi={S\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a0c6e9e0629d03c0bfec15575c55c598}{%
           family={Bronstein},
           familyi={B\bibinitperiod},
           given={Michael\bibnamedelima M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{43e47c48e339343c72620c9bf5e525a8}
      \strng{fullhash}{144d48be29262f8789c192115eb786b0}
      \strng{bibnamehash}{144d48be29262f8789c192115eb786b0}
      \strng{authorbibnamehash}{144d48be29262f8789c192115eb786b0}
      \strng{authornamehash}{43e47c48e339343c72620c9bf5e525a8}
      \strng{authorfullhash}{144d48be29262f8789c192115eb786b0}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.}
      \field{month}{11}
      \field{title}{Geometric deep learning on graphs and manifolds using mixture model CNNs}
      \field{year}{2016}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1611.08402
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1611.08402
      \endverb
    \endentry
    \entry{Paulheim2017}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=4fd4dfc5ed27cd65c69fa6d692f4c161}{%
           family={Paulheim},
           familyi={P\bibinitperiod},
           given={Heiko},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {IOS Press}%
      }
      \strng{namehash}{4fd4dfc5ed27cd65c69fa6d692f4c161}
      \strng{fullhash}{4fd4dfc5ed27cd65c69fa6d692f4c161}
      \strng{bibnamehash}{4fd4dfc5ed27cd65c69fa6d692f4c161}
      \strng{authorbibnamehash}{4fd4dfc5ed27cd65c69fa6d692f4c161}
      \strng{authornamehash}{4fd4dfc5ed27cd65c69fa6d692f4c161}
      \strng{authorfullhash}{4fd4dfc5ed27cd65c69fa6d692f4c161}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term Knowledge Graph in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.}
      \field{issn}{22104968}
      \field{issue}{3}
      \field{journaltitle}{Semantic Web}
      \field{title}{Knowledge graph refinement: A survey of approaches and evaluation methods}
      \field{volume}{8}
      \field{year}{2017}
      \field{pages}{489\bibrangedash 508}
      \range{pages}{20}
      \verb{doi}
      \verb 10.3233/SW-160218
      \endverb
      \keyw{Knowledge graphs,completion,correction,error detection,evaluation,refinement}
    \endentry
    \entry{Jorge2009}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=a991f832b9236d272602bb0cce8717ce}{%
           family={P√©rez},
           familyi={P\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9282d63c7c5bda72aad9b39bc68fddeb}{%
           family={Arenas},
           familyi={A\bibinitperiod},
           given={Marcelo},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5feae03b05f14f0a5fd1f3e07badd8e6}{%
           family={Gutierrez},
           familyi={G\bibinitperiod},
           given={Claudio},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d05db17dcccd9f3ea9a9bcb5a4019d18}
      \strng{fullhash}{9fca1e4ef52a32d40484eff4b3346ac3}
      \strng{bibnamehash}{9fca1e4ef52a32d40484eff4b3346ac3}
      \strng{authorbibnamehash}{9fca1e4ef52a32d40484eff4b3346ac3}
      \strng{authornamehash}{d05db17dcccd9f3ea9a9bcb5a4019d18}
      \strng{authorfullhash}{9fca1e4ef52a32d40484eff4b3346ac3}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{SPARQL is the standard language for querying RDF data. In this article, we address systematically the formal study of the database aspects of SPARQL, concentrating in its graph pattern matching facility. We provide a compositional semantics for the core part of SPARQL, and study the complexity of the evaluation of several fragments of the language. Among other complexity results, we show that the evaluation of general SPARQL patterns is PSPACE-complete. We identify a large class of SPARQL patterns, defined by imposing a simple and natural syntactic restriction, where the query evaluation problem can be solved more efficiently. This restriction gives rise to the class of well-designed patterns. We show that the evaluation problem is coNP-complete for well-designed patterns. Moreover, we provide several rewriting rules for well-designed patterns whose application may have a considerable impact in the cost of evaluating SPARQL queries. ¬© 2009 ACM.}
      \field{issn}{03625915}
      \field{issue}{3}
      \field{journaltitle}{ACM Transactions on Database Systems}
      \field{month}{8}
      \field{title}{Semantics and complexity of SPARQL}
      \field{volume}{34}
      \field{year}{2009}
      \verb{doi}
      \verb 10.1145/1567274.1567278
      \endverb
      \keyw{Complexity,Query language,RDF,SPARQL,Semantic Web}
    \endentry
    \entry{Perozzi2014}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=3bbe3b6d26e1f03feea22728836605cd}{%
           family={Perozzi},
           familyi={P\bibinitperiod},
           given={Bryan},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6fdea7728f40679d54135ffed015aec3}{%
           family={Al-Rfou},
           familyi={A\bibinithyphendelim R\bibinitperiod},
           given={Rami},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=14cc658e717a3f34fd056009828c501d}{%
           family={Skiena},
           familyi={S\bibinitperiod},
           given={Steven},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{42f913597cb4202ac7fbbeed7bad8273}
      \strng{fullhash}{75e48cd19787d1c675c8307f806c80a1}
      \strng{bibnamehash}{75e48cd19787d1c675c8307f806c80a1}
      \strng{authorbibnamehash}{75e48cd19787d1c675c8307f806c80a1}
      \strng{authornamehash}{42f913597cb4202ac7fbbeed7bad8273}
      \strng{authorfullhash}{75e48cd19787d1c675c8307f806c80a1}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9781450329569}
      \field{journaltitle}{Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}
      \field{title}{DeepWalk: Online learning of social representations}
      \field{year}{2014}
      \field{pages}{701\bibrangedash 710}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/2623330.2623732
      \endverb
      \keyw{deep learning,latent representations,learning with partial labels,network classification,online learning,social networks}
    \endentry
    \entry{pujara2013knowledge}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=603e425c04181fb9cb012633c322ac6d}{%
           family={Pujara},
           familyi={P\bibinitperiod},
           given={Jay},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=af92e5da928a8a416c661b6ca3829723}{%
           family={Miao},
           familyi={M\bibinitperiod},
           given={Hui},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=88e86bf57e4d0841ba9010d52469237c}{%
           family={Getoor},
           familyi={G\bibinitperiod},
           given={Lise},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=be006ecd16e3b55e58d8a895354f80e8}{%
           family={Cohen},
           familyi={C\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod},
           givenun=0}}%
      }
      \list{organization}{1}{%
        {Springer}%
      }
      \strng{namehash}{d8e1e5e0dd9bd1a7770745135bcd3796}
      \strng{fullhash}{6a3f87913ecc3f91b70606a7fc386a17}
      \strng{bibnamehash}{6a3f87913ecc3f91b70606a7fc386a17}
      \strng{authorbibnamehash}{6a3f87913ecc3f91b70606a7fc386a17}
      \strng{authornamehash}{d8e1e5e0dd9bd1a7770745135bcd3796}
      \strng{authorfullhash}{6a3f87913ecc3f91b70606a7fc386a17}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The Semantic Web--ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part I 12}
      \field{title}{Knowledge graph identification}
      \field{year}{2013}
      \field{pages}{542\bibrangedash 557}
      \range{pages}{16}
    \endentry
    \entry{Rong2019}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=f292ec47b3dca3814617cd5390916824}{%
           family={Rong},
           familyi={R\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0ee4909a832e993602aa62ad48918b29}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Wenbing},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bc4a291289dd8cc0a090f4585626d4f6}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Tingyang},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=10b0bc00e8e92345a8bc3f6c1e6bb625}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Junzhou},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{10e0b3852a636107e8d5fcb831c4c87b}
      \strng{fullhash}{4aa5968b8d3b105cfe35682390c30d27}
      \strng{bibnamehash}{4aa5968b8d3b105cfe35682390c30d27}
      \strng{authorbibnamehash}{4aa5968b8d3b105cfe35682390c30d27}
      \strng{authornamehash}{10e0b3852a636107e8d5fcb831c4c87b}
      \strng{authorfullhash}{4aa5968b8d3b105cfe35682390c30d27}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{\emph\{Over-fitting\} and \emph\{over-smoothing\} are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on~\url\{https://github.com/DropEdge/DropEdge\}.}
      \field{month}{7}
      \field{title}{DropEdge: Towards Deep Graph Convolutional Networks on Node Classification}
      \field{year}{2019}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.10903
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.10903
      \endverb
    \endentry
    \entry{Sahu2019}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=2ec66fed3d53dcb48e73c6087ddbdb83}{%
           family={Sahu},
           familyi={S\bibinitperiod},
           given={Sunil\bibnamedelima Kumar},
           giveni={S\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b2fdda8ea1fe86fd54269686b333e15e}{%
           family={Christopoulou},
           familyi={C\bibinitperiod},
           given={Fenia},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=178ae3a32e06734b86a89c12931c95c8}{%
           family={Miwa},
           familyi={M\bibinitperiod},
           given={Makoto},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3780b2d12b907a7196c92d8269005aba}{%
           family={Ananiadou},
           familyi={A\bibinitperiod},
           given={Sophia},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c2440ab99ce2fb832ad7182f7417fa53}
      \strng{fullhash}{770f770096b06d1c3b7c96f03f1fbff2}
      \strng{bibnamehash}{770f770096b06d1c3b7c96f03f1fbff2}
      \strng{authorbibnamehash}{770f770096b06d1c3b7c96f03f1fbff2}
      \strng{authornamehash}{c2440ab99ce2fb832ad7182f7417fa53}
      \strng{authorfullhash}{770f770096b06d1c3b7c96f03f1fbff2}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.}
      \field{month}{6}
      \field{title}{Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network}
      \field{year}{2019}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1906.04684
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1906.04684
      \endverb
    \endentry
    \entry{Scarselli2009}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=447de7983619243dc5bf3af69d609b20}{%
           family={Scarselli},
           familyi={S\bibinitperiod},
           given={Franco},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d25d4f83ae540b2b728a5423e9fbe85b}{%
           family={Gori},
           familyi={G\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d1abb4d2c7b43e8f49a10c2dfb0b3305}{%
           family={Tsoi},
           familyi={T\bibinitperiod},
           given={Ah\bibnamedelima Chung},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=10950c9e6d0e741519372f893853f1c9}{%
           family={Hagenbuchner},
           familyi={H\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b56d5f9b0efe692c5e81a88da3a2f2b7}{%
           family={Monfardini},
           familyi={M\bibinitperiod},
           given={Gabriele},
           giveni={G\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{2}{%
        {Institute of Electrical}%
        {Electronics Engineers Inc.}%
      }
      \strng{namehash}{e0bddd4250509618ebb1318d9844ce78}
      \strng{fullhash}{1ce29035cf9078f0d21a0ddd34800b9d}
      \strng{bibnamehash}{1ce29035cf9078f0d21a0ddd34800b9d}
      \strng{authorbibnamehash}{1ce29035cf9078f0d21a0ddd34800b9d}
      \strng{authornamehash}{e0bddd4250509618ebb1318d9844ce78}
      \strng{authorfullhash}{1ce29035cf9078f0d21a0ddd34800b9d}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function œÑ (G,n) ‚àà Rm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities. ¬© 2008 IEEE.}
      \field{issn}{10459227}
      \field{issue}{1}
      \field{journaltitle}{IEEE Transactions on Neural Networks}
      \field{month}{1}
      \field{title}{The graph neural network model}
      \field{volume}{20}
      \field{year}{2009}
      \field{pages}{61\bibrangedash 80}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1109/TNN.2008.2005605
      \endverb
      \keyw{Graph neural networks (GNNs),Graph processing,Graphical domains,Recursive neural networks}
    \endentry
    \entry{singhal2012introducing}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=e842c74a8f00853e7e3ea135b41b55bd}{%
           family={Singhal},
           familyi={S\bibinitperiod},
           given={Amit},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{edda2c0bbec7c54a7421c6e0ddaa8460}
      \strng{fullhash}{edda2c0bbec7c54a7421c6e0ddaa8460}
      \strng{bibnamehash}{edda2c0bbec7c54a7421c6e0ddaa8460}
      \strng{authorbibnamehash}{edda2c0bbec7c54a7421c6e0ddaa8460}
      \strng{authornamehash}{edda2c0bbec7c54a7421c6e0ddaa8460}
      \strng{authorfullhash}{edda2c0bbec7c54a7421c6e0ddaa8460}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Official google blog}
      \field{number}{16}
      \field{title}{Introducing the knowledge graph: things, not strings}
      \field{volume}{5}
      \field{year}{2012}
      \field{pages}{3}
      \range{pages}{1}
    \endentry
    \entry{Tang2015}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=541194db9dac70af45a28c067d289c8d}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=be51ffe8f55f70ee4360e3e6a1a4216e}{%
           family={Qu},
           familyi={Q\bibinitperiod},
           given={Meng},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a9b32eefcfd0b44a27bae162e66746ae}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Mingzhe},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7a56aedb5f6ad175a5340edc7bf6267e}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=934a0c3e849405255facd8113ba785b3}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2ca1c136fc4a8c4c39869eeaf7fade74}{%
           family={Mei},
           familyi={M\bibinitperiod},
           given={Qiaozhu},
           giveni={Q\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery, Inc}%
      }
      \strng{namehash}{46894c9a61813f6e656d9217e281e7d1}
      \strng{fullhash}{87054c2dc52ba2fab3bbd4445bd88471}
      \strng{bibnamehash}{87054c2dc52ba2fab3bbd4445bd88471}
      \strng{authorbibnamehash}{87054c2dc52ba2fab3bbd4445bd88471}
      \strng{authornamehash}{46894c9a61813f6e656d9217e281e7d1}
      \strng{authorfullhash}{87054c2dc52ba2fab3bbd4445bd88471}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9781450334693}
      \field{journaltitle}{WWW 2015 - Proceedings of the 24th International Conference on World Wide Web}
      \field{month}{5}
      \field{title}{LINE: Large-scale information network embedding}
      \field{year}{2015}
      \field{pages}{1067\bibrangedash 1077}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1145/2736277.2741093
      \endverb
      \keyw{Dimension Reduction,Feature Learning,Information Network Embedding,Scalability}
    \endentry
    \entry{Tchechmedjiev2019}{article}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=481c02172e78dc137bc6e575cb03fcd5}{%
           family={Tchechmedjiev},
           familyi={T\bibinitperiod},
           given={Andon},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4324253df183ab6a846df79383a55536}{%
           family={Fafalios},
           familyi={F\bibinitperiod},
           given={Pavlos},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2cf57ff30b9f417c2235cda77e31d15e}{%
           family={Boland},
           familyi={B\bibinitperiod},
           given={Katarina},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a118a87e7a14d90167424077d9b7e1e3}{%
           family={Gasquet},
           familyi={G\bibinitperiod},
           given={Malo},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=caa3026b5e4b05801dda814b3a7007b6}{%
           family={Zloch},
           familyi={Z\bibinitperiod},
           given={Matthaus},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f8c802debe23cd53409045a1997648cf}{%
           family={Zapilko},
           familyi={Z\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0eeb68cbb49d4e3b58ba0660fb2f6599}{%
           family={Dietze},
           familyi={D\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d33b076ff1dd5d96077bd88f62e61a84}{%
           family={Todorov},
           familyi={T\bibinitperiod},
           given={Konstantin},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=45a14a4ed27f4e27a824084c123e0d9c}{%
           family={Zloch},
           familyi={Z\bibinitperiod},
           given={Matth√§us},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{63171e2594d34852ec83ad2590430c39}
      \strng{fullhash}{86753f693720d99272cdcf4b5dbfa853}
      \strng{bibnamehash}{86753f693720d99272cdcf4b5dbfa853}
      \strng{authorbibnamehash}{86753f693720d99272cdcf4b5dbfa853}
      \strng{authornamehash}{63171e2594d34852ec83ad2590430c39}
      \strng{authorfullhash}{86753f693720d99272cdcf4b5dbfa853}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Various research areas at the intersection of computer and social sciences require a ground truth of contextualized claims labelled with their truth values in order to facilitate supervision, validation or reproducibility of approaches dealing, for example, with fact-checking or analysis of societal debates. So far, no reasonably large, up-to-date and queryable corpus of structured information about claims and related metadata is publicly available. In an attempt to fill this gap, we introduce ClaimsKG, a knowledge graph of fact-checked claims, which facilitates structured queries about their truth values, authors, dates, journalistic reviews and other kinds of metadata. ClaimsKG is generated through a semi-automated pipeline, which harvests data from popular fact-checking websites on a regular basis, annotates claims with related entities from DBpedia, and lifts the data to RDF using an RDF/S model that makes use of established vocabularies. In order to harmonise data originating from diverse fact-checking sites, we introduce normalised ratings as well as a simple claims coreference resolution strategy. The current knowledge graph, extensible to new information, consists of 28,383 claims published since 1996, amounting to 6,606,032 triples.}
      \field{title}{ClaimsKG: A Knowledge Graph of Fact-Checked Claims}
      \field{year}{2019}
      \field{pages}{309\bibrangedash 324}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1007/978-3-030-30796-7_20√Ø
      \endverb
      \verb{urlraw}
      \verb https://hal.science/hal-02404153
      \endverb
      \verb{url}
      \verb https://hal.science/hal-02404153
      \endverb
      \keyw{Claims,Fact-checking,Knowledge Graphs,Societal debates}
    \endentry
    \entry{Thekumparampil2018}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=442e9022a98c2b544bff8400f2b7aa10}{%
           family={Thekumparampil},
           familyi={T\bibinitperiod},
           given={Kiran\bibnamedelima K.},
           giveni={K\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5305db1dfd127efb3eb09c039837c18f}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Chong},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0a708b0573da1c8e751ef081a44490b9}{%
           family={Oh},
           familyi={O\bibinitperiod},
           given={Sewoong},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2afdae52015b97674d81efea449edce2}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Li-Jia},
           giveni={L\bibinithyphendelim J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{62b1ca1a536713bbe7846eb0209804f8}
      \strng{fullhash}{7fbd9672ed51e30e25c03a26e122bf7f}
      \strng{bibnamehash}{7fbd9672ed51e30e25c03a26e122bf7f}
      \strng{authorbibnamehash}{7fbd9672ed51e30e25c03a26e122bf7f}
      \strng{authornamehash}{62b1ca1a536713bbe7846eb0209804f8}
      \strng{authorfullhash}{7fbd9672ed51e30e25c03a26e122bf7f}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.}
      \field{month}{3}
      \field{title}{Attention-based Graph Neural Network for Semi-supervised Learning}
      \field{year}{2018}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1803.03735
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1803.03735
      \endverb
    \endentry
    \entry{Uschold1996}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=43ce48ece7fbfefcc05e550adb670c1d}{%
           family={Uschold},
           familyi={U\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=616604c7891ff52e572ad0cae8d13659}{%
           family={Gruninger},
           familyi={G\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a1df4b6446ee9900f00edd4f51b69bdb}
      \strng{fullhash}{a1df4b6446ee9900f00edd4f51b69bdb}
      \strng{bibnamehash}{a1df4b6446ee9900f00edd4f51b69bdb}
      \strng{authorbibnamehash}{a1df4b6446ee9900f00edd4f51b69bdb}
      \strng{authornamehash}{a1df4b6446ee9900f00edd4f51b69bdb}
      \strng{authorfullhash}{a1df4b6446ee9900f00edd4f51b69bdb}
      \field{sortinit}{U}
      \field{sortinithash}{6901a00e45705986ee5e7ca9fd39adca}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper is intended to serve as a comprehensive introduction to the emerging geld concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to eeective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. an√≤ntology') in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, rst discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing deenitions. We then consider the beneets of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the speciication, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging geld, considering various case studies, software tools for ontology development, key research issues and future prospects.}
      \field{issue}{2}
      \field{journaltitle}{Knowledge Engineering Review}
      \field{title}{Ontologies: Principles, Methods and Applications}
      \field{volume}{11}
      \field{year}{1996}
    \endentry
    \entry{Velickovic2018}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=b1208a71c9b067bcef8fb29c15a09b5d}{%
           family={Veliƒçkoviƒá},
           familyi={V\bibinitperiod},
           given={Petar},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c30bc5878e196747275a7ff581c6ed22}{%
           family={Cucurull},
           familyi={C\bibinitperiod},
           given={Guillem},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5a0765951987db22af274a31732111fa}{%
           family={Casanova},
           familyi={C\bibinitperiod},
           given={Arantxa},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8e0f06556535de7b7d9d04d390dc4ac3}{%
           family={Romero},
           familyi={R\bibinitperiod},
           given={Adriana},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b1dc447055a5e64105a2984bf8306851}{%
           family={Li√≤},
           familyi={L\bibinitperiod},
           given={Pietro},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a923132f6a9d7d93c40facfd855c7dc4}
      \strng{fullhash}{e36c8bf9f36dc60726c1c2afa402755c}
      \strng{bibnamehash}{e36c8bf9f36dc60726c1c2afa402755c}
      \strng{authorbibnamehash}{e36c8bf9f36dc60726c1c2afa402755c}
      \strng{authornamehash}{a923132f6a9d7d93c40facfd855c7dc4}
      \strng{authorfullhash}{e36c8bf9f36dc60726c1c2afa402755c}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).}
      \field{month}{10}
      \field{title}{Graph Attention Networks}
      \field{year}{2018}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1710.10903
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1710.10903
      \endverb
    \endentry
    \entry{Wang2017}{article}{}
      \name{author}{4}{}{%
        {{un=1,uniquepart=given,hash=e2068259fc5297e154f92a417136c922}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Quan},
           giveni={Q\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=dd2b0f5b4fe73348f6c0b8fdfe0ce772}{%
           family={Mao},
           familyi={M\bibinitperiod},
           given={Zhendong},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc58cd6a24a023ce7177d8f199d47875}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Bin},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cbbe0aee45b05ee2e216204167adfdac}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {IEEE Computer Society}%
      }
      \strng{namehash}{a4dce556535a9cd3d6fbcf51fe9c9a92}
      \strng{fullhash}{c9468d4be8f286ca970a8c0e0cc5de45}
      \strng{bibnamehash}{c9468d4be8f286ca970a8c0e0cc5de45}
      \strng{authorbibnamehash}{c9468d4be8f286ca970a8c0e0cc5de45}
      \strng{authornamehash}{a4dce556535a9cd3d6fbcf51fe9c9a92}
      \strng{authorfullhash}{c9468d4be8f286ca970a8c0e0cc5de45}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-The-Arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.}
      \field{issn}{10414347}
      \field{issue}{12}
      \field{journaltitle}{IEEE Transactions on Knowledge and Data Engineering}
      \field{month}{12}
      \field{title}{Knowledge graph embedding: A survey of approaches and applications}
      \field{volume}{29}
      \field{year}{2017}
      \field{pages}{2724\bibrangedash 2743}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1109/TKDE.2017.2754499
      \endverb
      \keyw{Knowledge graph embedding,Latent factor models,Statistical relational learning,Tensor/matrix factorization models}
    \endentry
    \entry{Wang2019}{article}{}
      \name{author}{5}{}{%
        {{un=1,uniquepart=given,hash=8fde8559617f603b3e2b3cf30cb48e9b}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xiang},
           giveni={X\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=cf3f1b6bfe8e9131a8c719c1af0dce5d}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Xiangnan},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=00b734eb3fb588e4c6c94e4271080d0b}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Meng},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9090db11da70fe0d8b95cbf3d73a6216}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Fuli},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2485f7cdced8c69ee540130a17724964}{%
           family={Chua},
           familyi={C\bibinitperiod},
           given={Tat\bibnamedelima Seng},
           giveni={T\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery, Inc}%
      }
      \strng{namehash}{35ac326fad78d42d0760cfb75887ea69}
      \strng{fullhash}{1b11f8d5d6c1713fec5d7a2c2aca4a3d}
      \strng{bibnamehash}{1b11f8d5d6c1713fec5d7a2c2aca4a3d}
      \strng{authorbibnamehash}{1b11f8d5d6c1713fec5d7a2c2aca4a3d}
      \strng{authornamehash}{35ac326fad78d42d0760cfb75887ea69}
      \strng{authorfullhash}{1b11f8d5d6c1713fec5d7a2c2aca4a3d}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect. In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.}
      \field{isbn}{9781450361729}
      \field{journaltitle}{SIGIR 2019 - Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval}
      \field{month}{7}
      \field{title}{Neural graph collaborative filtering}
      \field{year}{2019}
      \field{pages}{165\bibrangedash 174}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/3331184.3331267
      \endverb
      \keyw{Collaborative Filtering,Embedding Propagation,Graph Neural Network,High-order Connectivity,Recommendation}
    \endentry
    \entry{Wang2014}{misc}{}
      \name{author}{4}{}{%
        {{un=1,uniquepart=given,hash=d23f8780b31a9738eb2ec5f9c954bcb4}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Zhen},
           giveni={Z\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=8523cf05a337ae3f69b20974da48ce9a}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jianwen},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4435206f0ab50c344ea575c22c9465fb}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Jianlin},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=05694d0c858ee219eae7bb764b27b7f6}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zheng},
           giveni={Z\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{bcc0ee0115ca53333821bd83b79eadf2}
      \strng{fullhash}{75e9f72d345c40d5f36d96336a6504ea}
      \strng{bibnamehash}{75e9f72d345c40d5f36d96336a6504ea}
      \strng{authorbibnamehash}{75e9f72d345c40d5f36d96336a6504ea}
      \strng{authornamehash}{bcc0ee0115ca53333821bd83b79eadf2}
      \strng{authorfullhash}{75e9f72d345c40d5f36d96336a6504ea}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive , one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.}
      \field{title}{Knowledge Graph Embedding by Translating on Hyperplanes}
      \field{year}{2014}
      \verb{urlraw}
      \verb www.aaai.org
      \endverb
      \verb{url}
      \verb www.aaai.org
      \endverb
      \keyw{Knowledge Representation and Reasoning}
    \endentry
    \entry{Wu2021}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=c97b91ddf59fcb75f2d1b52080711566}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Zonghan},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=019bcbde9904047f7c02c1ae64e1d46e}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Shirui},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=60aaf7ecf6263904f7e30d9a299a044d}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Fengwen},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e7b6aa3c68f9430695b34429c0871856}{%
           family={Long},
           familyi={L\bibinitperiod},
           given={Guodong},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=59f3e845390faff6c808a1556b2ac0ef}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Chengqi},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ed7b2de2f08bfa6638d5d174d55d839b}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Philip\bibnamedelima S.},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{2}{%
        {Institute of Electrical}%
        {Electronics Engineers Inc.}%
      }
      \strng{namehash}{0d8fc377d7726031a0daf3b3b9ab64b3}
      \strng{fullhash}{0f1e0d108c1e93aa577ae493dc7dd45b}
      \strng{bibnamehash}{0f1e0d108c1e93aa577ae493dc7dd45b}
      \strng{authorbibnamehash}{0f1e0d108c1e93aa577ae493dc7dd45b}
      \strng{authornamehash}{0d8fc377d7726031a0daf3b3b9ab64b3}
      \strng{authorfullhash}{0f1e0d108c1e93aa577ae493dc7dd45b}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-The-Art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-Temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.}
      \field{issn}{21622388}
      \field{issue}{1}
      \field{journaltitle}{IEEE Transactions on Neural Networks and Learning Systems}
      \field{month}{1}
      \field{title}{A Comprehensive Survey on Graph Neural Networks}
      \field{volume}{32}
      \field{year}{2021}
      \field{pages}{4\bibrangedash 24}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1109/TNNLS.2020.2978386
      \endverb
      \keyw{Deep learning,graph autoencoder (GAE),graph convolutional networks (GCNs),graph neural networks (GNNs),graph representation learning,network embedding}
    \endentry
    \entry{Xu2019}{misc}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=29a3a6bf823a641230b0258e0a8a0214}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Keyulu},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=164c7f22cde069c57bf9b80f76441fa9}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Weihua},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=83a44c18acfaa38fa9f6ba76cfdc0132}{%
           family={Jegelka},
           familyi={J\bibinitperiod},
           given={Stefanie},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{344f5b90b6e6e45d86fe86fd8fdca38c}
      \strng{fullhash}{3fe1c2e12a449ac359ab2737dfe4ce17}
      \strng{bibnamehash}{3fe1c2e12a449ac359ab2737dfe4ce17}
      \strng{authorbibnamehash}{3fe1c2e12a449ac359ab2737dfe4ce17}
      \strng{authornamehash}{344f5b90b6e6e45d86fe86fd8fdca38c}
      \strng{authorfullhash}{3fe1c2e12a449ac359ab2737dfe4ce17}
      \field{sortinit}{X}
      \field{sortinithash}{1965c258adceecf23ce3d67b05113442}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.}
      \field{title}{HOW POWERFUL ARE GRAPH NEURAL NETWORKS?}
      \field{year}{2019}
    \endentry
    \entry{Yang_2018_ECCV}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=28f60884b33f4a669bad592a2b59e04d}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Jianwei},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6e96667037518d5907f881526f2af4f8}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Jiasen},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=571d2d1449f75b0c538caca8aa9bf4b6}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d0a408119272ca1bd49b625e3b927d05}{%
           family={Batra},
           familyi={B\bibinitperiod},
           given={Dhruv},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d0ab7a4601951a0c59352f1dcb531afb}{%
           family={Parikh},
           familyi={P\bibinitperiod},
           given={Devi},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b5d3b9b540e91aa30cfa6b84f7905f4b}
      \strng{fullhash}{15b95edb2cb63535463e854e6333c6d2}
      \strng{bibnamehash}{15b95edb2cb63535463e854e6333c6d2}
      \strng{authorbibnamehash}{15b95edb2cb63535463e854e6333c6d2}
      \strng{authornamehash}{b5d3b9b540e91aa30cfa6b84f7905f4b}
      \strng{authorfullhash}{15b95edb2cb63535463e854e6333c6d2}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Proceedings of the European Conference on Computer Vision (ECCV)}
      \field{month}{9}
      \field{title}{Graph R-CNN for Scene Graph Generation}
      \field{year}{2018}
    \endentry
    \entry{Yasunaga2021}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=f95714ad66afcfa71b317b3bec07d01d}{%
           family={Yasunaga},
           familyi={Y\bibinitperiod},
           given={Michihiro},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f136c99c06f10d114554179d4722b864}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Hongyu},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ed22853aa493ba4023443a010c9f6f8b}{%
           family={Bosselut},
           familyi={B\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86c032bbc45c3b616f0a1170befc0e82}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Percy},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e7e08a193d7fcd9f584a831f4e7ee906}
      \strng{fullhash}{31093619f9a2c6a54c9e56f98f98f488}
      \strng{bibnamehash}{31093619f9a2c6a54c9e56f98f98f488}
      \strng{authorbibnamehash}{31093619f9a2c6a54c9e56f98f98f488}
      \strng{authornamehash}{e7e08a193d7fcd9f584a831f4e7ee906}
      \strng{authorfullhash}{31093619f9a2c6a54c9e56f98f98f488}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.}
      \field{month}{4}
      \field{title}{QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering}
      \field{year}{2021}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2104.06378
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2104.06378
      \endverb
    \endentry
    \entry{Ying2018}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=6b37c8aefa150635663b2a6525952a7a}{%
           family={Ying},
           familyi={Y\bibinitperiod},
           given={Rex},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1b8a48b9ddb1c09f1c48bd467d99488a}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Ruining},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=806c56ba82f07f26518a4de5caf6b2b6}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kaifeng},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2146d3b9f777d15a77c7ff17474e9ad9}{%
           family={Eksombatchai},
           familyi={E\bibinitperiod},
           given={Pong},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2abce6c40b4cc1a3d89f175883b24536}{%
           family={Hamilton},
           familyi={H\bibinitperiod},
           given={William\bibnamedelima L.},
           giveni={W\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{abe60498875b44b92d8ff87a690002b1}
      \strng{fullhash}{7aa9b85fe91011148b5ed02b4ca83c49}
      \strng{bibnamehash}{7aa9b85fe91011148b5ed02b4ca83c49}
      \strng{authorbibnamehash}{7aa9b85fe91011148b5ed02b4ca83c49}
      \strng{authornamehash}{abe60498875b44b92d8ff87a690002b1}
      \strng{authorfullhash}{7aa9b85fe91011148b5ed02b4ca83c49}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.}
      \field{isbn}{9781450355520}
      \field{journaltitle}{Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}
      \field{month}{7}
      \field{title}{Graph convolutional neural networks for web-scale recommender systems}
      \field{year}{2018}
      \field{pages}{974\bibrangedash 983}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/3219819.3219890
      \endverb
    \endentry
    \entry{Zeng2019}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=46def165a2f198288abcb663a8e78ea2}{%
           family={Zeng},
           familyi={Z\bibinitperiod},
           given={Hanqing},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e29833ee1707ffa6672202116aaf88ed}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Hongkuan},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=39c6ce66605b1beb12aa301eb7b7d6a0}{%
           family={Srivastava},
           familyi={S\bibinitperiod},
           given={Ajitesh},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8c75dbe2e68808251371edb193c33d2e}{%
           family={Kannan},
           familyi={K\bibinitperiod},
           given={Rajgopal},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=276a91bbb61cbdced5c5649eafd6d5da}{%
           family={Prasanna},
           familyi={P\bibinitperiod},
           given={Viktor},
           giveni={V\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{9e59af265112a7a7f24e94ae87864826}
      \strng{fullhash}{c8c189742224f6f71275f1ec12047b7e}
      \strng{bibnamehash}{c8c189742224f6f71275f1ec12047b7e}
      \strng{authorbibnamehash}{c8c189742224f6f71275f1ec12047b7e}
      \strng{authornamehash}{9e59af265112a7a7f24e94ae87864826}
      \strng{authorfullhash}{c8c189742224f6f71275f1ec12047b7e}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the "neighbor explosion" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).}
      \field{month}{7}
      \field{title}{GraphSAINT: Graph Sampling Based Inductive Learning Method}
      \field{year}{2019}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.04931
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.04931
      \endverb
    \endentry
    \entry{Zhang2021}{article}{}
      \name{author}{8}{}{%
        {{un=0,uniquepart=base,hash=3fa0b98bdb0330cc0ea22e655fd964c2}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Wen},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=17cb5f39e75e54113cdf0eaa8f846568}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Shumin},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a921c7917808e511798fc1d34abd9af9}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Mingyang},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cce05cab2c437e9c128f853684ee3137}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Liang},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8b8805250fb5a57cfbae360547a18706}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Qiang},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3d1fce80b100a38e606bc7f2e0c6fccd}{%
           family={Xiong},
           familyi={X\bibinitperiod},
           given={Feiyu},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a8fe2d7a8f566f42e1585eb10778e6b5}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Xiangwen},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=824b2f383c5e03f7427dc3096f342870}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Huajun},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{6842b5fbca42689fb0276e5d0ca4a2a4}
      \strng{fullhash}{5229fdbbda4c36b762e6d44d3885780d}
      \strng{bibnamehash}{5229fdbbda4c36b762e6d44d3885780d}
      \strng{authorbibnamehash}{5229fdbbda4c36b762e6d44d3885780d}
      \strng{authornamehash}{6842b5fbca42689fb0276e5d0ca4a2a4}
      \strng{authorfullhash}{5229fdbbda4c36b762e6d44d3885780d}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge Graphs (KGs), representing facts as triples, have been widely adopted in many applications. Reasoning tasks such as link prediction and rule induction are important for the development of KGs. Knowledge Graph Embeddings (KGEs) embedding entities and relations of a KG into continuous vector spaces, have been proposed for these reasoning tasks and proven to be efficient and robust. But the plausibility and feasibility of applying and deploying KGEs in real-work applications has not been well-explored. In this paper, we discuss and report our experiences of deploying KGEs in a real domain application: e-commerce. We first identity three important desiderata for e-commerce KG systems: 1) attentive reasoning, reasoning over a few target relations of more concerns instead of all; 2) explanation, providing explanations for a prediction to help both users and business operators understand why the prediction is made; 3) transferable rules, generating reusable rules to accelerate the deployment of a KG to new systems. While non existing KGE could meet all these desiderata, we propose a novel one, an explainable knowledge graph attention network that make prediction through modeling correlations between triples rather than purely relying on its head entity, relation and tail entity embeddings. It could automatically selects attentive triples for prediction and records the contribution of them at the same time, from which explanations could be easily provided and transferable rules could be efficiently produced. We empirically show that our method is capable of meeting all three desiderata in our e-commerce application and outperform typical baselines on datasets from real domain applications.}
      \field{isbn}{9781450395656}
      \field{journaltitle}{ACM International Conference Proceeding Series}
      \field{month}{12}
      \field{title}{Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules}
      \field{year}{2021}
      \field{pages}{71\bibrangedash 79}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1145/3502223.3502232
      \endverb
      \keyw{E-commerce,Explainable AI,Knowledge Graphs,Reasoning,Representation Learning,Rules}
    \endentry
    \entry{Zhou2020}{misc}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=873b7b14d7a3c7290a8ad393066ee9a3}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Jie},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85909d8a0dae0f58235434d6a4c3a425}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Ganqu},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dbc192d3823d1022eb9ca28a4f81d27a}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Shengding},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=27c7fad66da172aaa64e08e3539ff172}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zhengyan},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=29ccf7aa9391a3e9e13ba3b57f4c48eb}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Cheng},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5b9a54b9d83697231299711961622ab8}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhiyuan},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b089f6bd0b31a94e61b42f89d966b1e1}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Lifeng},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d35819f1b7580b5294fc7d3dc3d24550}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Changcheng},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=faa6f5041a22004ca0dd547c03476790}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Maosong},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Elsevier B.V.}%
      }
      \strng{namehash}{52ac8b06238656e0c352b6e46f10dece}
      \strng{fullhash}{26b43ae59077cc1dba8f7bd57dea2b48}
      \strng{bibnamehash}{26b43ae59077cc1dba8f7bd57dea2b48}
      \strng{authorbibnamehash}{26b43ae59077cc1dba8f7bd57dea2b48}
      \strng{authornamehash}{52ac8b06238656e0c352b6e46f10dece}
      \strng{authorfullhash}{26b43ae59077cc1dba8f7bd57dea2b48}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.}
      \field{issn}{26666510}
      \field{journaltitle}{AI Open}
      \field{month}{1}
      \field{title}{Graph neural networks: A review of methods and applications}
      \field{volume}{1}
      \field{year}{2020}
      \field{pages}{57\bibrangedash 81}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1016/j.aiopen.2021.01.001
      \endverb
      \keyw{Deep learning,Graph neural network}
    \endentry
    \entry{Zou2020}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=7ff3406b3ef241e685952a0ffd371c74}{%
           family={Zou},
           familyi={Z\bibinitperiod},
           given={Xiaohan},
           giveni={X\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Institute of Physics Publishing}%
      }
      \strng{namehash}{7ff3406b3ef241e685952a0ffd371c74}
      \strng{fullhash}{7ff3406b3ef241e685952a0ffd371c74}
      \strng{bibnamehash}{7ff3406b3ef241e685952a0ffd371c74}
      \strng{authorbibnamehash}{7ff3406b3ef241e685952a0ffd371c74}
      \strng{authornamehash}{7ff3406b3ef241e685952a0ffd371c74}
      \strng{authorfullhash}{7ff3406b3ef241e685952a0ffd371c74}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \true{uniqueprimaryauthor}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Knowledge graphs, representation of information as a semantic graph, have caused wide concern in both industrial and academic world. Their property of providing semantically structured information has brought important possible solutions for many tasks including question answering, recommendation and information retrieval, and is considered to offer great promise for building more intelligent machines by many researchers. Although knowledge graphs have already supported multiple "Big Data" applications in all sorts of commercial and scientific domains since Google coined this term in 2012, there was no previous study give a systemically review of the application of knowledge graphs. Therefore, unlike other related work which focuses on the construction techniques of knowledge graphs, this present paper aims at providing a first survey on these applications stemming from different domains. This paper also points out that while important advancements of applying knowledge graphs' great ability of providing semantically structured information into specific domains have been made in recent years, several aspects still remain to be explored.}
      \field{issn}{17426596}
      \field{issue}{1}
      \field{journaltitle}{Journal of Physics: Conference Series}
      \field{month}{4}
      \field{title}{A Survey on Application of Knowledge Graph}
      \field{volume}{1487}
      \field{year}{2020}
      \verb{doi}
      \verb 10.1088/1742-6596/1487/1/012016
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

