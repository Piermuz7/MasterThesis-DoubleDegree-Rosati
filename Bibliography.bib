@article{Scarselli2009,
   abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function τ (G,n) ∈ Rm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities. © 2008 IEEE.},
   author = {Franco Scarselli and Marco Gori and Ah Chung Tsoi and Markus Hagenbuchner and Gabriele Monfardini},
   doi = {10.1109/TNN.2008.2005605},
   issn = {10459227},
   issue = {1},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {Graph neural networks (GNNs),Graph processing,Graphical domains,Recursive neural networks},
   month = {1},
   pages = {61-80},
   pmid = {19068426},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {The graph neural network model},
   volume = {20},
   year = {2009},
}
@article{Hogan2021,
   abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
   author = {Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia D'Amato and Gerard De Melo and Claudio Gutierrez and Sabrina Kirrane and José Emilio Labra Gayo and Roberto Navigli and Sebastian Neumaier and Axel Cyrille Ngonga Ngomo and Axel Polleres and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
   doi = {10.1145/3447772},
   issn = {15577341},
   issue = {4},
   journal = {ACM Computing Surveys},
   keywords = {Embeddings,Graph algorithms,Graph databases,Graph neural networks,Graph query languages,Knowledge graphs,Ontologies,Rule mining,Shapes},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Knowledge graphs},
   volume = {54},
   year = {2021},
}
@book{Antoniou2008,
   abstract = {2nd ed. The substantially updated second edition of a widely used guide to the key ideas, languages, and technologies of the Semantic Web, featuring additional coverage of new application areas, new tools, and other recent developments. Brief Contents -- Contents -- List of Figures -- Series Foreword -- Preface -- 1 The Semantic Web Vision -- 2 Structured Web Documents: XML -- 3 Describing Web Resources: RDF -- 4 Web Ontology Language: OWL -- 5 Logic and Inference: Rules -- 6 Applications -- 7 Ontology Engineering -- 8 Conclusion and Outlook -- A Abstract OWL Syntax -- Index},
   author = {{G. (Grigoris) Antoniou and Frank. Van Harmelen}},
   isbn = {9780262012423},
   pages = {264},
   publisher = {MIT Press},
   title = {A semantic Web primer},
   year = {2008},
}
@misc{Deborah2004,
   abstract = {The OWL Web Ontology Language is designed for use by applications that need to process the content of information instead of just presenting information to humans. OWL facilitates greater machine interpretability of Web content than that supported by XML, RDF, and RDF Schema (RDF-S) by providing additional vocabulary along with a formal semantics. OWL has three increasingly-expressive sublanguages: OWL Lite, OWL DL, and OWL Full.},
   author = {L. McGuinness Deborah and van Harmelen Frank},
   title = {OWL Web Ontology Language Overview},
   url = {http://www.w3.org/TR/2003/PR-owl-features-20031215/},
   year = {2004},
}
@article{Jorge2009,
   abstract = {SPARQL is the standard language for querying RDF data. In this article, we address systematically the formal study of the database aspects of SPARQL, concentrating in its graph pattern matching facility. We provide a compositional semantics for the core part of SPARQL, and study the complexity of the evaluation of several fragments of the language. Among other complexity results, we show that the evaluation of general SPARQL patterns is PSPACE-complete. We identify a large class of SPARQL patterns, defined by imposing a simple and natural syntactic restriction, where the query evaluation problem can be solved more efficiently. This restriction gives rise to the class of well-designed patterns. We show that the evaluation problem is coNP-complete for well-designed patterns. Moreover, we provide several rewriting rules for well-designed patterns whose application may have a considerable impact in the cost of evaluating SPARQL queries. © 2009 ACM.},
   author = {Jorge Pérez and Marcelo Arenas and Claudio Gutierrez},
   doi = {10.1145/1567274.1567278},
   issn = {03625915},
   issue = {3},
   journal = {ACM Transactions on Database Systems},
   keywords = {Complexity,Query language,RDF,SPARQL,Semantic Web},
   month = {8},
   title = {Semantics and complexity of SPARQL},
   volume = {34},
   year = {2009},
}
@article{Kapanipathi2020,
   abstract = {Knowledge base question answering (KBQA)is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity andrelationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems.},
   author = {Pavan Kapanipathi and Ibrahim Abdelaziz and Srinivas Ravishankar and Salim Roukos and Alexander Gray and Ramon Astudillo and Maria Chang and Cristina Cornelio and Saswati Dana and Achille Fokoue and Dinesh Garg and Alfio Gliozzo and Sairam Gurajada and Hima Karanam and Naweed Khan and Dinesh Khandelwal and Young-Suk Lee and Yunyao Li and Francois Luus and Ndivhuwo Makondo and Nandana Mihindukulasooriya and Tahira Naseem and Sumit Neelam and Lucian Popa and Revanth Reddy and Ryan Riegel and Gaetano Rossiello and Udit Sharma and G P Shrivatsa Bhargav and Mo Yu},
   month = {12},
   title = {Leveraging Abstract Meaning Representation for Knowledge Base Question Answering},
   url = {http://arxiv.org/abs/2012.01707},
   year = {2020},
}
@article{Tchechmedjiev2019,
   abstract = {Various research areas at the intersection of computer and social sciences require a ground truth of contextualized claims labelled with their truth values in order to facilitate supervision, validation or reproducibility of approaches dealing, for example, with fact-checking or analysis of societal debates. So far, no reasonably large, up-to-date and queryable corpus of structured information about claims and related metadata is publicly available. In an attempt to fill this gap, we introduce ClaimsKG, a knowledge graph of fact-checked claims, which facilitates structured queries about their truth values, authors, dates, journalistic reviews and other kinds of metadata. ClaimsKG is generated through a semi-automated pipeline, which harvests data from popular fact-checking websites on a regular basis, annotates claims with related entities from DBpedia, and lifts the data to RDF using an RDF/S model that makes use of established vocabularies. In order to harmonise data originating from diverse fact-checking sites, we introduce normalised ratings as well as a simple claims coreference resolution strategy. The current knowledge graph, extensible to new information, consists of 28,383 claims published since 1996, amounting to 6,606,032 triples.},
   author = {Andon Tchechmedjiev and Pavlos Fafalios and Katarina Boland and Malo Gasquet and Matthaus Zloch and Benjamin Zapilko and Stefan Dietze and Konstantin Todorov and Matthäus Zloch},
   doi = {10.1007/978-3-030-30796-7_20ï},
   keywords = {Claims,Fact-checking,Knowledge Graphs,Societal debates},
   pages = {309-324},
   title = {ClaimsKG: A Knowledge Graph of Fact-Checked Claims},
   url = {https://hal.science/hal-02404153},
   year = {2019},
}
@inproceedings{Zhang2021,
   abstract = {Knowledge Graphs (KGs), representing facts as triples, have been widely adopted in many applications. Reasoning tasks such as link prediction and rule induction are important for the development of KGs. Knowledge Graph Embeddings (KGEs) embedding entities and relations of a KG into continuous vector spaces, have been proposed for these reasoning tasks and proven to be efficient and robust. But the plausibility and feasibility of applying and deploying KGEs in real-work applications has not been well-explored. In this paper, we discuss and report our experiences of deploying KGEs in a real domain application: e-commerce. We first identity three important desiderata for e-commerce KG systems: 1) attentive reasoning, reasoning over a few target relations of more concerns instead of all; 2) explanation, providing explanations for a prediction to help both users and business operators understand why the prediction is made; 3) transferable rules, generating reusable rules to accelerate the deployment of a KG to new systems. While non existing KGE could meet all these desiderata, we propose a novel one, an explainable knowledge graph attention network that make prediction through modeling correlations between triples rather than purely relying on its head entity, relation and tail entity embeddings. It could automatically selects attentive triples for prediction and records the contribution of them at the same time, from which explanations could be easily provided and transferable rules could be efficiently produced. We empirically show that our method is capable of meeting all three desiderata in our e-commerce application and outperform typical baselines on datasets from real domain applications.},
   author = {Wen Zhang and Shumin Deng and Mingyang Chen and Liang Wang and Qiang Chen and Feiyu Xiong and Xiangwen Liu and Huajun Chen},
   doi = {10.1145/3502223.3502232},
   isbn = {9781450395656},
   journal = {ACM International Conference Proceeding Series},
   keywords = {E-commerce,Explainable AI,Knowledge Graphs,Reasoning,Representation Learning,Rules},
   month = {12},
   pages = {71-79},
   publisher = {Association for Computing Machinery},
   title = {Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules},
   year = {2021},
}
@inproceedings{pujara2013knowledge,
  title={Knowledge graph identification},
  author={Pujara, Jay and Miao, Hui and Getoor, Lise and Cohen, William},
  booktitle={The Semantic Web--ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part I 12},
  pages={542--557},
  year={2013},
  organization={Springer}
}
@article{singhal2012introducing,
  title={Introducing the knowledge graph: things, not strings},
  author={Singhal, Amit and others},
  journal={Official google blog},
  volume={5},
  number={16},
  pages={3},
  year={2012}
}
@article{Wang2017,
   abstract = {Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-The-Arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
   author = {Quan Wang and Zhendong Mao and Bin Wang and Li Guo},
   doi = {10.1109/TKDE.2017.2754499},
   issn = {10414347},
   issue = {12},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {Knowledge graph embedding,Latent factor models,Statistical relational learning,Tensor/matrix factorization models},
   month = {12},
   pages = {2724-2743},
   publisher = {IEEE Computer Society},
   title = {Knowledge graph embedding: A survey of approaches and applications},
   volume = {29},
   year = {2017},
}
@article{Wu2021,
   abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-The-Art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-Temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
   author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
   doi = {10.1109/TNNLS.2020.2978386},
   issn = {21622388},
   issue = {1},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep learning,graph autoencoder (GAE),graph convolutional networks (GCNs),graph neural networks (GNNs),graph representation learning,network embedding},
   month = {1},
   pages = {4-24},
   pmid = {32217482},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Comprehensive Survey on Graph Neural Networks},
   volume = {32},
   year = {2021},
}
@article{Yasunaga2021,
   abstract = {The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.},
   author = {Michihiro Yasunaga and Hongyu Ren and Antoine Bosselut and Percy Liang and Jure Leskovec},
   month = {4},
   title = {QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering},
   url = {http://arxiv.org/abs/2104.06378},
   year = {2021},
}
@article{Fernandez2011,
   abstract = {Currently, techniques for content description and query processing in Information Retrieval (IR) are based on keywords, and therefore provide limited capabilities to capture the conceptualizations associated with user needs and contents. Aiming to solve the limitations of keyword-based models, the idea of conceptual search, understood as searching by meanings rather than literal strings, has been the focus of a wide body of research in the IR field. More recently, it has been used as a prototypical scenario (or even envisioned as a potential "killer app") in the Semantic Web (SW) vision, since its emergence in the late nineties. However, current approaches to semantic search developed in the SW area have not yet taken full advantage of the acquired knowledge, accumulated experience, and technological sophistication achieved through several decades of work in the IR field. Starting from this position, this work investigates the definition of an ontology-based IR model, oriented to the exploitation of domain Knowledge Bases to support semantic search capabilities in large document repositories, stressing on the one hand the use of fully fledged ontologies in the semantic-based perspective, and on the other hand the consideration of unstructured content as the target search space. The major contribution of this work is an innovative, comprehensive semantic search model, which extends the classic IR model, addresses the challenges of the massive and heterogeneous Web environment, and integrates the benefits of both keyword and semantic-based search. Additional contributions include: an innovative rank fusion technique that minimizes the undesired effects of knowledge sparseness on the yet juvenile SW, and the creation of a large-scale evaluation benchmark, based on TREC IR evaluation standards, which allows a rigorous comparison between IR and SW approaches. Conducted experiments show that our semantic search model obtained comparable and better performance results (in terms of MAP and P@10 values) than the best TREC automatic system. © 2010 Elsevier B.V. All rights reserved.},
   author = {Miriam Fernández and Iván Cantador and Vanesa López and David Vallet and Pablo Castells and Enrico Motta},
   doi = {10.1016/j.websem.2010.11.003},
   issn = {15708268},
   issue = {4},
   journal = {Journal of Web Semantics},
   keywords = {Information Retrieval,Semantic Web,Semantic search},
   month = {12},
   pages = {434-452},
   title = {Semantically enhanced Information Retrieval: An ontology-based approach},
   volume = {9},
   year = {2011},
}
@article{Paulheim2017,
   abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term Knowledge Graph in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
   author = {Heiko Paulheim},
   doi = {10.3233/SW-160218},
   issn = {22104968},
   issue = {3},
   journal = {Semantic Web},
   keywords = {Knowledge graphs,completion,correction,error detection,evaluation,refinement},
   pages = {489-508},
   publisher = {IOS Press},
   title = {Knowledge graph refinement: A survey of approaches and evaluation methods},
   volume = {8},
   year = {2017},
}
@InProceedings{Bonatti2017,
author="Bonatti, Piero and Kirrane, Sabrina and Polleres, Axel and Wenning, Rigo",
editor="Tonetta, Stefano and Schoitsch, Erwin and Bitsch, Friedemann",
title="Transparent Personal Data Processing: The Road Ahead",
booktitle="Computer Safety, Reliability, and Security ",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="337--349",
abstract="The European General Data Protection Regulation defines a set of obligations for personal data controllers and processors. Primary obligations include: obtaining explicit consent from the data subject for the processing of personal data, providing full transparency with respect to the processing, and enabling data rectification and erasure (albeit only in certain circumstances). At the core of any transparency architecture is the logging of events in relation to the processing and sharing of personal data. The logs should enable verification that data processors abide by the access and usage control policies that have been associated with the data based on the data subject's consent and the applicable regulations. In this position paper, we: (i) identify the requirements that need to be satisfied by such a transparency architecture, (ii) examine the suitability of existing logging mechanisms in light of said requirements, and (iii) present a number of open challenges and opportunities.",
isbn="978-3-319-66284-8"
}
@inbook{Bizer2023,
author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
title = {Linked Data - The Story So Far},
year = {2023},
isbn = {9798400707940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3591366.3591378},
booktitle = {Linking the World’s Information: Essays on Tim Berners-Lee’s Invention of the World Wide Web},
pages = {115–143},
numpages = {29}
}
@misc{Uschold1996,
   abstract = {This paper is intended to serve as a comprehensive introduction to the emerging geld concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to eeective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. anòntology') in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, rst discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing deenitions. We then consider the beneets of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the speciication, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging geld, considering various case studies, software tools for ontology development, key research issues and future prospects.},
   author = {Mike Uschold and Michael Gruninger},
   issue = {2},
   journal = {Knowledge Engineering Review},
   title = {Ontologies: Principles, Methods and Applications},
   volume = {11},
   year = {1996},
}
@inproceedings{Francis2018,
   abstract = {The Cypher property graph query language is an evolving language, originally designed and implemented as part of the Neo4j graph database, and it is currently used by several commercial database products and researchers. We describe Cypher 9, which is the first version of the language governed by the openCypher Implementers Group. We first introduce the language by example, and describe its uses in industry. We then provide a formal semantic definition of the core read-query features of Cypher, including its variant of the property graph data model, and its "ASCII Art" graph pattern matching mechanism for expressing subgraphs of interest to an application. We compare the features of Cypher to other property graph query languages, and describe extensions, at an advanced stage of development, which will form part of Cypher 10, turning the language into a compositional language which supports graph projections and multiple named graphs.},
   author = {Nadime Francis and Alastair Green and Paolo Guagliardo and Leonid Libkin and Tobias Lindaaker and Victor Marsault and Stefan Plantikow and Mats Rydberg and Petra Selmer and Andrés Taylor},
   doi = {10.1145/3183713.3190657},
   isbn = {9781450317436},
   issn = {07308078},
   journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
   month = {5},
   pages = {1433-1445},
   publisher = {Association for Computing Machinery},
   title = {Cypher: An evolving query language for property graphs},
   year = {2018},
}
@article{Bruna2013,
   abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
   author = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
   month = {12},
   title = {Spectral Networks and Locally Connected Networks on Graphs},
   url = {http://arxiv.org/abs/1312.6203},
   year = {2013},
}
@misc{Defferrard2016,
   abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
   author = {Michaël Defferrard and Xavier Bresson and Pierre Vandergheynst},
   title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
   url = {https://github.com/mdeff/cnn_graph},
   year = {2016},
}
@article{Kipf2017,
   abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
   author = {Thomas N. Kipf and Max Welling},
   month = {9},
   title = {Semi-Supervised Classification with Graph Convolutional Networks},
   url = {http://arxiv.org/abs/1609.02907},
   year = {2017},
}
@article{Velickovic2018,
   abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
   author = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
   month = {10},
   title = {Graph Attention Networks},
   url = {http://arxiv.org/abs/1710.10903},
   year = {2017},
}
@misc{Hamilton2017,
   abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
   author = {William L Hamilton and Rex Ying and Jure Leskovec},
   title = {Inductive Representation Learning on Large Graphs},
   year = {2017},
}
@misc{Gilmer2017,
   abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery , and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper , we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
   author = {Justin Gilmer and Samuel S Schoenholz and Patrick F Riley and Oriol Vinyals and George E Dahl},
   title = {Neural Message Passing for Quantum Chemistry},
   year = {2017},
}
@article{Chen2017,
   abstract = {Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.},
   author = {Jianfei Chen and Jun Zhu and Le Song},
   month = {10},
   title = {Stochastic Training of Graph Convolutional Networks with Variance Reduction},
   url = {http://arxiv.org/abs/1710.10568},
   year = {2017},
}
@article{Zeng2019,
   abstract = {Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the "neighbor explosion" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).},
   author = {Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},
   month = {7},
   title = {GraphSAINT: Graph Sampling Based Inductive Learning Method},
   url = {http://arxiv.org/abs/1907.04931},
   year = {2019},
}
@misc{Fout2017,
   abstract = {We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.},
   author = {Alex Fout and Jonathon Byrd and Basir Shariat and Asa Ben-Hur},
   title = {Protein Interface Prediction using Graph Convolutional Networks},
   year = {2017},
}
@misc{Jin2018,
   abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model out-performs previous state-of-the-art baselines by a significant margin.},
   author = {Wengong Jin and Regina Barzilay and Tommi Jaakkola},
   title = {Junction Tree Variational Autoencoder for Molecular Graph Generation},
   year = {2018},
}
@inproceedings{Wang2019,
   abstract = {Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect. In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.},
   author = {Xiang Wang and Xiangnan He and Meng Wang and Fuli Feng and Tat Seng Chua},
   doi = {10.1145/3331184.3331267},
   isbn = {9781450361729},
   journal = {SIGIR 2019 - Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {Collaborative Filtering,Embedding Propagation,Graph Neural Network,High-order Connectivity,Recommendation},
   month = {7},
   pages = {165-174},
   publisher = {Association for Computing Machinery, Inc},
   title = {Neural graph collaborative filtering},
   year = {2019},
}
@inproceedings{Ying2018,
   abstract = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
   author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and Jure Leskovec},
   doi = {10.1145/3219819.3219890},
   isbn = {9781450355520},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   month = {7},
   pages = {974-983},
   publisher = {Association for Computing Machinery},
   title = {Graph convolutional neural networks for web-scale recommender systems},
   year = {2018},
}
@article{Sahu2019,
   abstract = {Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.},
   author = {Sunil Kumar Sahu and Fenia Christopoulou and Makoto Miwa and Sophia Ananiadou},
   month = {6},
   title = {Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network},
   url = {http://arxiv.org/abs/1906.04684},
   year = {2019},
}
@inproceedings{Chiang2019,
   abstract = {Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy-using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16].},
   author = {Wei Lin Chiang and Yang Li and Xuanqing Liu and Samy Bengio and Si Si and Cho Jui Hsieh},
   doi = {10.1145/3292500.3330925},
   isbn = {9781450362016},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   month = {7},
   pages = {257-266},
   publisher = {Association for Computing Machinery},
   title = {Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks},
   year = {2019},
}
@article{manic2016building,
  title={Building energy management systems: The age of intelligent and adaptive buildings},
  author={Manic, Milos and Wijayasekara, Dumidu and Amarasinghe, Kasun and Rodriguez-Andina, Juan J},
  journal={IEEE Industrial Electronics Magazine},
  volume={10},
  number={1},
  pages={25--39},
  year={2016},
  publisher={IEEE}
}
@TechReport{Cyganiak14RCA,
  author= {Richard Cyganiak and Markus Lanthaler and David Wood},
  title= {{RDF} 1.1 Concepts and Abstract Syntax},
  month= {feb},
  url = {https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/},
  year= {2014},
  bibsource= {https://w2.syronex.com/jmr/w3c-biblio},
  type= {W3C Recommendation},
  institution= {W3C},
}
@misc{Kejriwal2022,
   abstract = {Knowledge graphs (KGs) have rapidly emerged as an important area in AI over the last ten years. Building on a storied tradition of graphs in the AI community, a KG may be simply defined as a directed, labeled, multi-relational graph with some form of semantics. In part, this has been fueled by increased publication of structured datasets on the Web, and well-publicized successes of large-scale projects such as the Google Knowledge Graph and the Amazon Product Graph. However, another factor that is less discussed, but which has been equally instrumental in the success of KGs, is the cross-disciplinary nature of academic KG research. Arguably, because of the diversity of this research, a synthesis of how different KG research strands all tie together could serve a useful role in enabling more ‘moonshot’ research and large-scale collaborations. This review of the KG research landscape attempts to provide such a synthesis by first showing what the major strands of research are, and how those strands map to different communities, such as Natural Language Processing, Databases and Semantic Web. A unified framework is suggested in which to view the distinct, but overlapping, foci of KG research within these communities.},
   author = {Mayank Kejriwal},
   doi = {10.3390/info13040161},
   issn = {20782489},
   issue = {4},
   journal = {Information (Switzerland)},
   keywords = {applications,data mining,graph databases,knowledge graphs,knowledge representation,natural language processing,semantic web},
   month = {4},
   publisher = {MDPI},
   title = {Knowledge Graphs: A Practical Review of the Research Landscape},
   volume = {13},
   year = {2022},
}
@inproceedings{Zou2020,
   abstract = {Knowledge graphs, representation of information as a semantic graph, have caused wide concern in both industrial and academic world. Their property of providing semantically structured information has brought important possible solutions for many tasks including question answering, recommendation and information retrieval, and is considered to offer great promise for building more intelligent machines by many researchers. Although knowledge graphs have already supported multiple "Big Data" applications in all sorts of commercial and scientific domains since Google coined this term in 2012, there was no previous study give a systemically review of the application of knowledge graphs. Therefore, unlike other related work which focuses on the construction techniques of knowledge graphs, this present paper aims at providing a first survey on these applications stemming from different domains. This paper also points out that while important advancements of applying knowledge graphs' great ability of providing semantically structured information into specific domains have been made in recent years, several aspects still remain to be explored.},
   author = {Xiaohan Zou},
   doi = {10.1088/1742-6596/1487/1/012016},
   issn = {17426596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   month = {4},
   publisher = {Institute of Physics Publishing},
   title = {A Survey on Application of Knowledge Graph},
   volume = {1487},
   year = {2020},
}
@misc{Bordes2013,
   abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
   author = {Antoine Bordes and Nicolas Usunier and Alberto Garcia-Durán and Jason Weston and Oksana Yakhnenko},
   title = {Translating Embeddings for Modeling Multi-relational Data},
   year = {2013},
}
@misc{Wang2014,
   abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive , one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
   author = {Zhen Wang and Jianwen Zhang and Jianlin Feng and Zheng Chen},
   keywords = {Knowledge Representation and Reasoning},
   title = {Knowledge Graph Embedding by Translating on Hyperplanes},
   url = {www.aaai.org},
   year = {2014},
}
@misc{Lin2015,
   abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments , we evaluate our models on three tasks including link prediction, triple classification and rela-tional fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.},
   author = {Yankai Lin and Zhiyuan Liu and Maosong Sun and Yang Liu and Xuan Zhu},
   keywords = {NLP and Knowledge Representation Track},
   title = {Learning Entity and Relation Embeddings for Knowledge Graph Completion},
   url = {www.aaai.org},
   year = {2015},
}
@article{Chaudhri2022,
   abstract = {Knowledge graphs (KGs) have emerged as a compelling abstraction for organizing the world’s structured knowledge and for integrating information extracted from multiple data sources. They are also beginning to play a central role in representing information extracted by AI systems, and for improving the predictions of AI systems by giving them knowledge expressed in KGs as input. The goals of this article are to (a) introduce KGs and discuss important areas of application that have gained recent prominence; (b) situate KGs in the context of the prior work in AI; and (c) present a few contrasting perspectives that help in better understanding KGs in relation to related technologies.},
   author = {Vinay K. Chaudhri and Chaitanya Baru and Naren Chittar and Xin Luna Dong and Michael Genesereth and James Hendler and Aditya Kalyanpur and Douglas B. Lenat and Juan Sequeda and Denny Vrandečić and Kuansan Wang},
   doi = {10.1002/aaai.12033},
   issn = {23719621},
   issue = {1},
   journal = {AI Magazine},
   month = {3},
   pages = {17-29},
   publisher = {John Wiley and Sons Inc},
   title = {Knowledge graphs: Introduction, history, and perspectives},
   volume = {43},
   year = {2022},
}
@misc{Zhou2020,
   abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
   author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
   doi = {10.1016/j.aiopen.2021.01.001},
   issn = {26666510},
   journal = {AI Open},
   keywords = {Deep learning,Graph neural network},
   month = {1},
   pages = {57-81},
   publisher = {Elsevier B.V.},
   title = {Graph neural networks: A review of methods and applications},
   volume = {1},
   year = {2020},
}