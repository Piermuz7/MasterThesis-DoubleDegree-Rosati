@article{Scarselli2009,
   abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function τ (G,n) ∈ Rm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities. © 2008 IEEE.},
   author = {Franco Scarselli and Marco Gori and Ah Chung Tsoi and Markus Hagenbuchner and Gabriele Monfardini},
   doi = {10.1109/TNN.2008.2005605},
   issn = {10459227},
   issue = {1},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {Graph neural networks (GNNs),Graph processing,Graphical domains,Recursive neural networks},
   month = {1},
   pages = {61-80},
   pmid = {19068426},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {The graph neural network model},
   volume = {20},
   year = {2009},
}
@article{Hogan2021,
   abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
   author = {Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia D'Amato and Gerard De Melo and Claudio Gutierrez and Sabrina Kirrane and José Emilio Labra Gayo and Roberto Navigli and Sebastian Neumaier and Axel Cyrille Ngonga Ngomo and Axel Polleres and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
   doi = {10.1145/3447772},
   issn = {15577341},
   issue = {4},
   journal = {ACM Computing Surveys},
   keywords = {Embeddings,Graph algorithms,Graph databases,Graph neural networks,Graph query languages,Knowledge graphs,Ontologies,Rule mining,Shapes},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Knowledge graphs},
   volume = {54},
   year = {2021},
}
@book{Antoniou2008,
   abstract = {2nd ed. The substantially updated second edition of a widely used guide to the key ideas, languages, and technologies of the Semantic Web, featuring additional coverage of new application areas, new tools, and other recent developments. Brief Contents -- Contents -- List of Figures -- Series Foreword -- Preface -- 1 The Semantic Web Vision -- 2 Structured Web Documents: XML -- 3 Describing Web Resources: RDF -- 4 Web Ontology Language: OWL -- 5 Logic and Inference: Rules -- 6 Applications -- 7 Ontology Engineering -- 8 Conclusion and Outlook -- A Abstract OWL Syntax -- Index},
   author = {{G. (Grigoris) Antoniou and Frank. Van Harmelen}},
   isbn = {9780262012423},
   pages = {264},
   publisher = {MIT Press},
   title = {A semantic Web primer},
   year = {2008},
}
@misc{Deborah2004,
   abstract = {The OWL Web Ontology Language is designed for use by applications that need to process the content of information instead of just presenting information to humans. OWL facilitates greater machine interpretability of Web content than that supported by XML, RDF, and RDF Schema (RDF-S) by providing additional vocabulary along with a formal semantics. OWL has three increasingly-expressive sublanguages: OWL Lite, OWL DL, and OWL Full.},
   author = {L. McGuinness Deborah and van Harmelen Frank},
   title = {OWL Web Ontology Language Overview},
   url = {http://www.w3.org/TR/2003/PR-owl-features-20031215/},
   year = {2004},
}
@article{Jorge2009,
   abstract = {SPARQL is the standard language for querying RDF data. In this article, we address systematically the formal study of the database aspects of SPARQL, concentrating in its graph pattern matching facility. We provide a compositional semantics for the core part of SPARQL, and study the complexity of the evaluation of several fragments of the language. Among other complexity results, we show that the evaluation of general SPARQL patterns is PSPACE-complete. We identify a large class of SPARQL patterns, defined by imposing a simple and natural syntactic restriction, where the query evaluation problem can be solved more efficiently. This restriction gives rise to the class of well-designed patterns. We show that the evaluation problem is coNP-complete for well-designed patterns. Moreover, we provide several rewriting rules for well-designed patterns whose application may have a considerable impact in the cost of evaluating SPARQL queries. © 2009 ACM.},
   author = {Jorge Pérez and Marcelo Arenas and Claudio Gutierrez},
   doi = {10.1145/1567274.1567278},
   issn = {03625915},
   issue = {3},
   journal = {ACM Transactions on Database Systems},
   keywords = {Complexity,Query language,RDF,SPARQL,Semantic Web},
   month = {8},
   title = {Semantics and complexity of SPARQL},
   volume = {34},
   year = {2009},
}
@article{Kapanipathi2020,
   abstract = {Knowledge base question answering (KBQA)is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity andrelationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems.},
   author = {Pavan Kapanipathi and Ibrahim Abdelaziz and Srinivas Ravishankar and Salim Roukos and Alexander Gray and Ramon Astudillo and Maria Chang and Cristina Cornelio and Saswati Dana and Achille Fokoue and Dinesh Garg and Alfio Gliozzo and Sairam Gurajada and Hima Karanam and Naweed Khan and Dinesh Khandelwal and Young-Suk Lee and Yunyao Li and Francois Luus and Ndivhuwo Makondo and Nandana Mihindukulasooriya and Tahira Naseem and Sumit Neelam and Lucian Popa and Revanth Reddy and Ryan Riegel and Gaetano Rossiello and Udit Sharma and G P Shrivatsa Bhargav and Mo Yu},
   month = {12},
   title = {Leveraging Abstract Meaning Representation for Knowledge Base Question Answering},
   url = {http://arxiv.org/abs/2012.01707},
   year = {2020},
}
@article{Tchechmedjiev2019,
   abstract = {Various research areas at the intersection of computer and social sciences require a ground truth of contextualized claims labelled with their truth values in order to facilitate supervision, validation or reproducibility of approaches dealing, for example, with fact-checking or analysis of societal debates. So far, no reasonably large, up-to-date and queryable corpus of structured information about claims and related metadata is publicly available. In an attempt to fill this gap, we introduce ClaimsKG, a knowledge graph of fact-checked claims, which facilitates structured queries about their truth values, authors, dates, journalistic reviews and other kinds of metadata. ClaimsKG is generated through a semi-automated pipeline, which harvests data from popular fact-checking websites on a regular basis, annotates claims with related entities from DBpedia, and lifts the data to RDF using an RDF/S model that makes use of established vocabularies. In order to harmonise data originating from diverse fact-checking sites, we introduce normalised ratings as well as a simple claims coreference resolution strategy. The current knowledge graph, extensible to new information, consists of 28,383 claims published since 1996, amounting to 6,606,032 triples.},
   author = {Andon Tchechmedjiev and Pavlos Fafalios and Katarina Boland and Malo Gasquet and Matthaus Zloch and Benjamin Zapilko and Stefan Dietze and Konstantin Todorov and Matthäus Zloch},
   doi = {10.1007/978-3-030-30796-7_20ï},
   keywords = {Claims,Fact-checking,Knowledge Graphs,Societal debates},
   pages = {309-324},
   title = {ClaimsKG: A Knowledge Graph of Fact-Checked Claims},
   url = {https://hal.science/hal-02404153},
   year = {2019},
}
@inproceedings{Zhang2021,
   abstract = {Knowledge Graphs (KGs), representing facts as triples, have been widely adopted in many applications. Reasoning tasks such as link prediction and rule induction are important for the development of KGs. Knowledge Graph Embeddings (KGEs) embedding entities and relations of a KG into continuous vector spaces, have been proposed for these reasoning tasks and proven to be efficient and robust. But the plausibility and feasibility of applying and deploying KGEs in real-work applications has not been well-explored. In this paper, we discuss and report our experiences of deploying KGEs in a real domain application: e-commerce. We first identity three important desiderata for e-commerce KG systems: 1) attentive reasoning, reasoning over a few target relations of more concerns instead of all; 2) explanation, providing explanations for a prediction to help both users and business operators understand why the prediction is made; 3) transferable rules, generating reusable rules to accelerate the deployment of a KG to new systems. While non existing KGE could meet all these desiderata, we propose a novel one, an explainable knowledge graph attention network that make prediction through modeling correlations between triples rather than purely relying on its head entity, relation and tail entity embeddings. It could automatically selects attentive triples for prediction and records the contribution of them at the same time, from which explanations could be easily provided and transferable rules could be efficiently produced. We empirically show that our method is capable of meeting all three desiderata in our e-commerce application and outperform typical baselines on datasets from real domain applications.},
   author = {Wen Zhang and Shumin Deng and Mingyang Chen and Liang Wang and Qiang Chen and Feiyu Xiong and Xiangwen Liu and Huajun Chen},
   doi = {10.1145/3502223.3502232},
   isbn = {9781450395656},
   journal = {ACM International Conference Proceeding Series},
   keywords = {E-commerce,Explainable AI,Knowledge Graphs,Reasoning,Representation Learning,Rules},
   month = {12},
   pages = {71-79},
   publisher = {Association for Computing Machinery},
   title = {Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules},
   year = {2021},
}
@inproceedings{pujara2013knowledge,
  title={Knowledge graph identification},
  author={Pujara, Jay and Miao, Hui and Getoor, Lise and Cohen, William},
  booktitle={The Semantic Web--ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part I 12},
  pages={542--557},
  year={2013},
  organization={Springer}
}
@article{singhal2012introducing,
  title={Introducing the knowledge graph: things, not strings},
  author={Singhal, Amit and others},
  journal={Official google blog},
  volume={5},
  number={16},
  pages={3},
  year={2012}
}
@article{Wang2017,
   abstract = {Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-The-Arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
   author = {Quan Wang and Zhendong Mao and Bin Wang and Li Guo},
   doi = {10.1109/TKDE.2017.2754499},
   issn = {10414347},
   issue = {12},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {Knowledge graph embedding,Latent factor models,Statistical relational learning,Tensor/matrix factorization models},
   month = {12},
   pages = {2724-2743},
   publisher = {IEEE Computer Society},
   title = {Knowledge graph embedding: A survey of approaches and applications},
   volume = {29},
   year = {2017},
}
@article{Wu2021,
   abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-The-Art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-Temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
   author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
   doi = {10.1109/TNNLS.2020.2978386},
   issn = {21622388},
   issue = {1},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep learning,graph autoencoder (GAE),graph convolutional networks (GCNs),graph neural networks (GNNs),graph representation learning,network embedding},
   month = {1},
   pages = {4-24},
   pmid = {32217482},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Comprehensive Survey on Graph Neural Networks},
   volume = {32},
   year = {2021},
}
@article{Yasunaga2021,
   abstract = {The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.},
   author = {Michihiro Yasunaga and Hongyu Ren and Antoine Bosselut and Percy Liang and Jure Leskovec},
   month = {4},
   title = {QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering},
   url = {http://arxiv.org/abs/2104.06378},
   year = {2021},
}
@article{Fernandez2011,
   abstract = {Currently, techniques for content description and query processing in Information Retrieval (IR) are based on keywords, and therefore provide limited capabilities to capture the conceptualizations associated with user needs and contents. Aiming to solve the limitations of keyword-based models, the idea of conceptual search, understood as searching by meanings rather than literal strings, has been the focus of a wide body of research in the IR field. More recently, it has been used as a prototypical scenario (or even envisioned as a potential "killer app") in the Semantic Web (SW) vision, since its emergence in the late nineties. However, current approaches to semantic search developed in the SW area have not yet taken full advantage of the acquired knowledge, accumulated experience, and technological sophistication achieved through several decades of work in the IR field. Starting from this position, this work investigates the definition of an ontology-based IR model, oriented to the exploitation of domain Knowledge Bases to support semantic search capabilities in large document repositories, stressing on the one hand the use of fully fledged ontologies in the semantic-based perspective, and on the other hand the consideration of unstructured content as the target search space. The major contribution of this work is an innovative, comprehensive semantic search model, which extends the classic IR model, addresses the challenges of the massive and heterogeneous Web environment, and integrates the benefits of both keyword and semantic-based search. Additional contributions include: an innovative rank fusion technique that minimizes the undesired effects of knowledge sparseness on the yet juvenile SW, and the creation of a large-scale evaluation benchmark, based on TREC IR evaluation standards, which allows a rigorous comparison between IR and SW approaches. Conducted experiments show that our semantic search model obtained comparable and better performance results (in terms of MAP and P@10 values) than the best TREC automatic system. © 2010 Elsevier B.V. All rights reserved.},
   author = {Miriam Fernández and Iván Cantador and Vanesa López and David Vallet and Pablo Castells and Enrico Motta},
   doi = {10.1016/j.websem.2010.11.003},
   issn = {15708268},
   issue = {4},
   journal = {Journal of Web Semantics},
   keywords = {Information Retrieval,Semantic Web,Semantic search},
   month = {12},
   pages = {434-452},
   title = {Semantically enhanced Information Retrieval: An ontology-based approach},
   volume = {9},
   year = {2011},
}
@article{Paulheim2017,
   abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term Knowledge Graph in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
   author = {Heiko Paulheim},
   doi = {10.3233/SW-160218},
   issn = {22104968},
   issue = {3},
   journal = {Semantic Web},
   keywords = {Knowledge graphs,completion,correction,error detection,evaluation,refinement},
   pages = {489-508},
   publisher = {IOS Press},
   title = {Knowledge graph refinement: A survey of approaches and evaluation methods},
   volume = {8},
   year = {2017},
}
@InProceedings{Bonatti2017,
author="Bonatti, Piero and Kirrane, Sabrina and Polleres, Axel and Wenning, Rigo",
editor="Tonetta, Stefano and Schoitsch, Erwin and Bitsch, Friedemann",
title="Transparent Personal Data Processing: The Road Ahead",
booktitle="Computer Safety, Reliability, and Security ",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="337--349",
abstract="The European General Data Protection Regulation defines a set of obligations for personal data controllers and processors. Primary obligations include: obtaining explicit consent from the data subject for the processing of personal data, providing full transparency with respect to the processing, and enabling data rectification and erasure (albeit only in certain circumstances). At the core of any transparency architecture is the logging of events in relation to the processing and sharing of personal data. The logs should enable verification that data processors abide by the access and usage control policies that have been associated with the data based on the data subject's consent and the applicable regulations. In this position paper, we: (i) identify the requirements that need to be satisfied by such a transparency architecture, (ii) examine the suitability of existing logging mechanisms in light of said requirements, and (iii) present a number of open challenges and opportunities.",
isbn="978-3-319-66284-8"
}
@inbook{Bizer2023,
author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
title = {Linked Data - The Story So Far},
year = {2023},
isbn = {9798400707940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3591366.3591378},
booktitle = {Linking the World’s Information: Essays on Tim Berners-Lee’s Invention of the World Wide Web},
pages = {115–143},
numpages = {29}
}
@misc{Uschold1996,
   abstract = {This paper is intended to serve as a comprehensive introduction to the emerging geld concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to eeective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. anòntology') in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, rst discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing deenitions. We then consider the beneets of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the speciication, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging geld, considering various case studies, software tools for ontology development, key research issues and future prospects.},
   author = {Mike Uschold and Michael Gruninger},
   issue = {2},
   journal = {Knowledge Engineering Review},
   title = {Ontologies: Principles, Methods and Applications},
   volume = {11},
   year = {1996},
}
@inproceedings{Francis2018,
   abstract = {The Cypher property graph query language is an evolving language, originally designed and implemented as part of the Neo4j graph database, and it is currently used by several commercial database products and researchers. We describe Cypher 9, which is the first version of the language governed by the openCypher Implementers Group. We first introduce the language by example, and describe its uses in industry. We then provide a formal semantic definition of the core read-query features of Cypher, including its variant of the property graph data model, and its "ASCII Art" graph pattern matching mechanism for expressing subgraphs of interest to an application. We compare the features of Cypher to other property graph query languages, and describe extensions, at an advanced stage of development, which will form part of Cypher 10, turning the language into a compositional language which supports graph projections and multiple named graphs.},
   author = {Nadime Francis and Alastair Green and Paolo Guagliardo and Leonid Libkin and Tobias Lindaaker and Victor Marsault and Stefan Plantikow and Mats Rydberg and Petra Selmer and Andrés Taylor},
   doi = {10.1145/3183713.3190657},
   isbn = {9781450317436},
   issn = {07308078},
   journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
   month = {5},
   pages = {1433-1445},
   publisher = {Association for Computing Machinery},
   title = {Cypher: An evolving query language for property graphs},
   year = {2018},
}
